import os
import sys
import json
import time
import shutil
import asyncio
import base64
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re
import html
import logging

from typing import Any, Dict, List
from pathlib import Path
from PIL import Image
import plotly.express as px

# from contextlib import AsyncExitStack

# from mcp.client.stdio import stdio_client
# from mcp import ClientSession, StdioServerParameters
# from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# =========================
# 0) ê³µí†µ ì„¤ì •
# =========================
ASSETS = Path("assets")
st.set_page_config(
    page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬",
    layout="wide",
    page_icon="ğŸ“ˆ",
)

PLOTTING_NUMERIC_COLS = [
    'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨', 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨',
    'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘', 'ê±°ì£¼ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'
]

@st.cache_data
def load_image(name: str):
    return Image.open(ASSETS / name)

# í—¤ë”
st.title("ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ ğŸ”‘")
st.image(load_image("KMWL.png"), width='stretch', caption="ê³ ë¯¼í•˜ì§€ ë§ê³ , AIë¹„ë°€ìƒë‹´ì‚¬ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!")
st.write("")

# ë¡œê¹…: í™”ë©´ ëŒ€ì‹  ì½˜ì†”ì— ì¶œë ¥, ë‚´ë¶€ ì‘ë™ ê¸°ë¡ì„ ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

def user_warn(msg: str):
    st.warning(msg, icon="âš ï¸")

def user_err(msg: str):
    st.error(msg, icon="ğŸ›‘")

# (í˜„ì¬ ì½”ë“œ ìœ ì§€â€”í•˜ë“œì½”ë“œ)
GOOGLE_API_KEY = "AIzaSyB8R3nurDOohfAvKXSgBUVRkoliXtfnTKo"
genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel("gemini-2.5-flash")
async_model = genai.GenerativeModel("gemini-2.5-flash")

# =========================
# 1) Streamlitìš© ë¡œê±°
# =========================
class StreamlitLogHandler:
    """
    Streamlit ì„¸ì…˜ ìƒíƒœ(session_state)ì— ë¡œê·¸ë¥¼ ê¸°ë¡í•˜ê³ ,
    ì œê³µëœ UI ì»¨í…Œì´ë„ˆì— ë¡œê·¸ë¥¼ ê·¸ë¦¬ëŠ” ì—­í• ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    def __init__(self):
        if "log_entries" not in st.session_state:
            st.session_state.log_entries = []

    def log(self, message: str, details: dict = None, expander_label: str = "ìƒì„¸ ê²°ê³¼ ë³´ê¸°", update: bool = False):
        """ì„¸ì…˜ ìƒíƒœì— ìƒˆë¡œìš´ ë¡œê·¸ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë§ˆì§€ë§‰ ë¡œê·¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        entry = {"message": f"- {message}", "details": details, "expander_label": expander_label}
        if update and st.session_state.log_entries:
            st.session_state.log_entries[-1] = entry
        else:
            st.session_state.log_entries.append(entry)
        time.sleep(0.5)

    def render(self, container):
        """ì œê³µëœ ì»¨í…Œì´ë„ˆì— í˜„ì¬ê¹Œì§€ì˜ ëª¨ë“  ë¡œê·¸ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
        container.empty()
        for entry in st.session_state.log_entries:
            container.markdown(entry["message"])
            if entry["details"]:
                with container.expander(entry["expander_label"], expanded=False):
                    st.json(entry["details"])

# ---------------------------
# ì—ì´ì „íŠ¸(ì¶”ë¡ /ë¦¬í¬íŒ…) ë¡œì§
# ---------------------------
class InteractiveParallelAgent:
    """
    - ë³€í™˜ â†’ ë¬¸ì œì •ì˜ â†’ ì „ëµ â†’ (í†µí•©)ìµœì¢… ë³´ê³ ì„œ
    - ë³´ê³ ì„œëŠ” í†µí•© í”„ë¡¬í”„íŠ¸(_GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT) ì‚¬ìš©
    - ì™¸ë¶€(run_full_pipeline)ì—ì„œ xploration(ì‹œê³„ì—´) ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
    """
        
    # 1) ë³€í™˜
    _TRANSFORM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë„¤ ê°€ì§€ JSON í•­ëª©ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- "target": ì‚¬ìš©ìê°€ ì „ëµì„ ìš”ì²­í•œ ëŒ€ìƒì€ ë¬´ì—‡ì¸ê°€ìš”? (ê°€ë§¹ì  ì´ë¦„. ì˜ˆ: 'ì„±ìš°**', 'í–‰*') ('ë§¤ì¥', 'ê°€ë§¹ì 'ê³¼ ê°™ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”)
- "challenge": ì‚¬ìš©ìê°€ ì§ë©´í•œ ì–´ë ¤ì›€ì´ë‚˜ í•´ê²°í•´ì•¼ í•  ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
- "objective": ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
- "solution_direction": ì œì•ˆë  ìˆ˜ ìˆëŠ” í•´ê²°ì±…ì˜ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
"""

    # 2) ë¬¸ì œ ì •ì˜
    _DEFINE_PROBLEM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê°€ë§¹ì ì˜ ì••ì¶•ëœ í†µê³„ ìš”ì•½ê³¼ ìµœì´ˆ ì‚¬ìš©ì ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ, ë¬¸ì œ ìƒí™©ê³¼ í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.

**ë°ì´í„° êµ¬ì¡° ì„¤ëª…:**
ì œê³µëœ ë°ì´í„°ëŠ” ì‹œê³„ì—´ í†µê³„ ìš”ì•½ìœ¼ë¡œ, ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
1. ê¸°ë³¸ ì •ë³´ (meta): ê°€ë§¹ì ID, ì—…ì¢…, ìƒê¶Œ, ë¶„ì„ê¸°ê°„, cluster ì •ë³´
2. ì „ì²´ í‰ê·  ì§€í‘œ (avg_metrics): 24ê°œì›” í‰ê· ê°’
3. íŠ¸ë Œë“œ (trend): ì´ˆê¸° 6ê°œì›” vs ìµœê·¼ 6ê°œì›” ë¹„êµ
4. ìµœê·¼ 3ê°œì›” ìƒì„¸ (recent_3months): ì›”ë³„ ìƒì„¸ ë°ì´í„°
5. ì£¼ìš” ì´ë²¤íŠ¸ (significant_events): ê¸‰ê²©í•œ ë³€í™” ì‹œì 
6. êµ¬ê°„ ë¹ˆë„ (range_frequency): ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì„±ê³¼ êµ¬ê°„

**ë°ì´í„° í•´ì„ ê°€ì´ë“œ:**
- ë§¤ì¶œê¸ˆì•¡/ê±´ìˆ˜/ê³ ê°ìˆ˜: ì‹¤ì œ ê°’
- ì·¨ì†Œìœ¨: ì‹¤ì œ ë¹„ìœ¨
- ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡/ê±´ìˆ˜ ë¹„ìœ¨: 100% ê¸°ì¤€, 100% ì´ìƒì´ë©´ í‰ê·  ì´ìƒ
- ë™ì¼ ì—…ì¢…/ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨: 0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„ê¶Œ
- cluster: 0(ìƒìœ„ê¶Œ ì•ˆì •), 1(í•˜ìœ„ê¶Œ ì•ˆì •), 2(ë³€ë™ì„± ë†’ìŒ), 3(ì‹ ê·œ ê°€ë§¹ì )
- cluster_6: 0(ì—¬ì„±40ëŒ€+ê±°ì£¼), 1(ë‚¨ì„±30-40ëŒ€+ì‹ ê·œ), 2(2030+ì‹ ê·œ+ìœ ë™), 3(60ëŒ€ì´ìƒ), 4(ë‚¨ì„±40-60ëŒ€), 5(ì§ì¥+ë‚¨ì„±30ëŒ€+ì¬ë°©ë¬¸)

**íŠ¹íˆ ì£¼ëª©í•  ì :**
- íŠ¸ë Œë“œì˜ ë³€í™”ìœ¨(ì´ˆê¸° ëŒ€ë¹„ ìµœê·¼)
- ìµœê·¼ 3ê°œì›”ì˜ ì›”ë³„ ì¶”ì„¸
- ì£¼ìš” ì´ë²¤íŠ¸(ê¸‰ë“±/ê¸‰ë½) ì‹œì ê³¼ í¬ê¸°

ê²°ê³¼ëŠ” ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- "problem_statement": í˜„ì¬ ì§ë©´í•œ êµ¬ì²´ì ì¸ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”? (íŠ¸ë Œë“œì™€ ìµœê·¼ ë°ì´í„° ê¸°ë°˜)
- "kpis": ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¸¡ì • ê°€ëŠ¥í•œ í•µì‹¬ ì§€í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”? (ë¬¸ìì—´ ëª©ë¡)
"""

    # 4) ì „ëµ ì œì•ˆ
    _GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì´ì ì „ë¬¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 'ë¬¸ì œ ì •ì˜'ì™€ ì••ì¶•ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, ìµœì¢… ì‚¬ìš©ìë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ë³´ê³ ì„œëŠ” ë‹¤ìŒì˜ ëª…í™•í•œ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì„œë¡ : ë¬¸ì œ ë°°ê²½ ë° ë¶„ì„ ëª©ì ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë³¸ë¡ :

ë°ì´í„° ê¸°ë°˜ í˜„í™© ë¶„ì„: íŠ¸ë Œë“œì™€ ìµœê·¼ 3ê°œì›” ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•©ë‹ˆë‹¤.

í•µì‹¬ ë¬¸ì œ ì •ì˜: ë¶„ì„ì„ í†µí•´ ë„ì¶œëœ ê°€ì¥ ì‹œê¸‰í•˜ê³  ì¤‘ìš”í•œ ë¬¸ì œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

êµ¬ì²´ì ì¸ í•´ê²° ì „ëµ: ì •ì˜ëœ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµê³¼ ì„¸ë¶€ ì‹¤í–‰ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ì™œ ì´ ì „ëµì´ íš¨ê³¼ì ì¼ì§€ì— ëŒ€í•œ ë…¼ë¦¬ì  ê·¼ê±°ë¥¼ ë°ì´í„° ì¶”ì„¸ì™€ ì—°ê²°í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.

ê²°ë¡ : ì œì•ˆëœ ì „ëµ ì‹¤í–‰ ì‹œ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ "report"ë¼ëŠ” ë‹¨ì¼ í‚¤ë¥¼ ê°€ì§„ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ê°’ì—ëŠ” ì „ì²´ ë³´ê³ ì„œ ë‚´ìš©ì´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ì˜ˆì‹œ: {"report": "## ìµœì¢… ë³´ê³ ì„œ\n\n### 1. ê°œìš”\n...\n### 2. í˜„í™© ë¶„ì„ ë° ë¬¸ì œ ì •ì˜\nìµœê·¼ 3ê°œì›”ê°„ ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤...\n### 3. í•´ê²° ì „ëµ\n...\n"}
"""

    def __init__(self, df: pd.DataFrame, model):
        self.df = df
        self.model = model
        self.context = {} # ë¶„ì„ ê³¼ì •ì„ ì €ì¥í•  ì»¨í…ìŠ¤íŠ¸

    def _generate_content_sync(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        LLM í˜¸ì¶œì„ ë‹´ë‹¹í•˜ëŠ” ì¤‘ì•™ ë©”ì„œë“œ. max_output_tokens ë“± ë‹¤ì–‘í•œ ì˜µì…˜ ì§€ì›
        """
        try:
            # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: generation_configë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
            generation_config = {"response_mime_type": "application/json"}
            if 'max_output_tokens' in kwargs:
                generation_config['max_output_tokens'] = kwargs['max_output_tokens']

            resp = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            return json.loads(resp.text)
        except Exception as e:
            return {"error": f"LLM ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"}

    def transform(self, initial_input: str):
        user = f"ğŸ”¹ ì‚¬ìš©ì ì…ë ¥: {initial_input}"
        full = f"{self._TRANSFORM_SYSTEM_PROMPT}\n\n{user}"
        self.context["transformation"] = self._generate_content_sync(full)
        return self.context["transformation"]
    
    def set_target_store(self, merchant_id: str):
        """
        ë¶„ì„í•  ëŒ€ìƒ ê°€ë§¹ì ì˜ IDë¥¼ ë°›ì•„ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥
        """
        store_data = self.df[self.df['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸'].astype(str) == str(merchant_id)]
        if store_data.empty:
            self.context['error'] = "í•´ë‹¹ IDì˜ ê°€ë§¹ì  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return
        # ë¶„ì„ì— ì‚¬ìš©ë  ê°€ë§¹ì ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
        self.context['target_store_timeseries'] = store_data.sort_values(by='ê¸°ì¤€ë…„ì›”').reset_index(drop=True)

    def compress_store_data(self) -> Dict[str, Any]:
        """
        3ë‹¨ê³„: ê°€ë§¹ì  ë°ì´í„° ì••ì¶• ìš”ì•½
        """
        print("3ë‹¨ê³„: ë°ì´í„° ì••ì¶• ìš”ì•½ ì‹œì‘...")
        store_data = self.context.get('target_store_timeseries')
        if store_data is None or store_data.empty:
            self.context['error'] = "ì••ì¶•í•  ê°€ë§¹ì  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            return {"error": self.context['error']}

        # --- (ì‚¬ìš©ìê°€ ì œê³µí•œ ì••ì¶• ë¡œì§ ì‹œì‘) ---
        first_row = store_data.iloc[0]
        meta = {
            'ê°€ë§¹ì ID': str(first_row.get('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸')), 'ê°€ë§¹ì ëª…': first_row.get('ê°€ë§¹ì ëª…'),
            'ì—…ì¢…': first_row.get('ì—…ì¢…'), 'ìƒê¶Œ': first_row.get('ìƒê¶Œ'),
            'ë¶„ì„ê¸°ê°„': f"{store_data['ê¸°ì¤€ë…„ì›”'].min().strftime('%Y-%m')} ~ {store_data['ê¸°ì¤€ë…„ì›”'].max().strftime('%Y-%m')}",
            'ë°ì´í„°í¬ì¸íŠ¸': len(store_data),
            'cluster': int(first_row['cluster']) if pd.notna(first_row.get('cluster')) else None,
            'cluster_6': int(first_row['cluster_6']) if pd.notna(first_row.get('cluster_6')) else None
        }
        numeric_cols = [
            'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨','ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨', 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨',
            'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘', 'ê±°ì£¼ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'
        ] # ìˆ˜ì¹˜í˜• ë³€ìˆ˜
        
        store_data_clean = store_data.copy()
        for col in numeric_cols:
            if col in store_data_clean.columns:
                store_data_clean[col] = pd.to_numeric(store_data_clean[col], errors='coerce')
        
        avg_metrics = {col: round(store_data_clean[col].mean(), 2) for col in numeric_cols if col in store_data_clean.columns and store_data_clean[col].notna().any()}
        
        first_6 = store_data_clean.head(6)
        last_6 = store_data_clean.tail(6)
        trend = {}
        for col in numeric_cols:
            if col in store_data_clean.columns:
                first_val, last_val = first_6[col].mean(), last_6[col].mean()
                if pd.notna(first_val) and pd.notna(last_val) and first_val != 0:
                    change_pct = ((last_val - first_val) / abs(first_val)) * 100
                    trend[col] = {'ì´ˆê¸°_í‰ê· ': round(first_val, 2), 'ìµœê·¼_í‰ê· ': round(last_val, 2), 'ë³€í™”ìœ¨': f"{change_pct:+.1f}%"}

        exploration_result = {'meta': meta, 'avg_metrics': avg_metrics, 'trend': trend}
        self.context['exploration'] = exploration_result
        print("3ë‹¨ê³„: ë°ì´í„° ì••ì¶• ì™„ë£Œ.")
        return exploration_result
    
    def define_problem(self) -> Dict[str, Any]:
        """
         4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ (ì••ì¶• ë°ì´í„°, max_tokens ì‚¬ìš©)
        """
        print("4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì‹œì‘...")
        exploration = self.context.get('exploration')
        transformation = self.context.get('transformation')
        if not exploration or not transformation:
            return {"error": "ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
        user_prompt = f"""
ğŸ”¹ ë¬¸ë§¥(Context):
- ìµœì´ˆ ìš”ì²­: {json.dumps(transformation, indent=2, ensure_ascii=False)}
- ì••ì¶•ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼: {json.dumps(exploration, indent=2, ensure_ascii=False)}
"""
        full_prompt = f"{self._DEFINE_PROBLEM_SYSTEM_PROMPT}\n\n{user_prompt}"
        
        response = self._generate_content_sync(
            full_prompt,
            max_output_tokens=4096  # âœ… max_tokens ì ìš©
        )
        self.context['problem_definition'] = response
        print("4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì™„ë£Œ.")
        return response

    def generate_report(self) -> str:
        """5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
        compact_context = {
            "transformation": self.context.get("transformation"),
            "problem_definition": self.context.get("problem_definition"),
            "exploration_summary": self.context.get("exploration"),
        }
        user_prompt = "ğŸ”¹ ì „ì²´ ì»¨í…ìŠ¤íŠ¸:\n" + json.dumps(compact_context, ensure_ascii=False, indent=2)
        full_prompt = f"{self._GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT}\n\n{user_prompt}"

        response = self._generate_content_sync(full_prompt,
        max_output_tokens=8192, # (ì°¸ê³ ) Flash ëª¨ë¸ì˜ ìµœëŒ€ ì¶œë ¥ì€ 8192 í† í°ì…ë‹ˆë‹¤.
        temperature=0.3
    )
        
        if "report" in response:
            print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
            return response["report"]
        else:
            print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨ (Fallback).")
            # Fallback: ì‹¤íŒ¨ ì‹œì—ë„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì—¬ì¤Œ
            return f"# ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨\n\nAIê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„° ìš”ì•½ì…ë‹ˆë‹¤.\n\n```json\n{json.dumps(compact_context, ensure_ascii=False, indent=2)}\n```"

# ---------------------------
# Streamlit UI
# ---------------------------
system_prompt = (
    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , "
    "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•ê³¼ ì±„ë„, ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. "
    "ê²°ê³¼ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ, ë¶„ì„ ê²°ê³¼ì—ëŠ” ê°€ëŠ¥í•œ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
greeting = "ê°€ë§¹ì  ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜, ê³ ë¯¼ì„ ë§ì”€í•´ì£¼ì„¸ìš”. AIë¹„ë°€ìƒë‹´ì‚¬ê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"

def ensure_state():
    ss = st.session_state
    if "messages" not in ss:
        ss.messages = [SystemMessage(content=system_prompt), AIMessage(content=greeting)]
    
    # âœ… íŒŒì´í”„ë¼ì¸ì˜ ëª¨ë“  í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”
    if "pipeline" not in ss:
        ss.pipeline = {
            "running": False, 
            "step": "idle",
            "user_query": "",
            "trans": None,
            "merchant_id": None,
            "exploration": None, # ì¶”ê°€ ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸
            "pd_out": None,      # ì¶”ê°€ ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸
            "report_md": None
        }
    ss.setdefault("awaiting_candidate", False)
    ss.setdefault("candidates", [])

ensure_state()

def clear_chat_history():
    st.session_state.messages = [
        SystemMessage(content=system_prompt),
        AIMessage(content=greeting),
    ]
    st.session_state.awaiting_candidate = False
    st.session_state.candidates = []
    st.session_state.exploration = None
    st.session_state.transformation = None
    st.session_state.selected_merchant_id = None
    st.session_state.user_query = ""
    # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    st.session_state.pipeline = {
        "running": False, "cancel_requested": False, "step": "idle",
        "merchant_id": None, "user_query": "", "trans": None,
        "candidates": [], "exploration": None, "pd_out": None,
        "report_md": None,
    }

def handle_followup(user_question: str) -> str:
    """
    í˜„ì¬ ì„¸ì…˜ì˜ exploration / transformation / reportë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•´
    ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. ìƒˆ íŒŒì´í”„ë¼ì¸ì„ ëŒë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    exp = st.session_state.pipeline.get("exploration")
    trans = st.session_state.pipeline.get("trans")
    report_md = st.session_state.pipeline.get("report_md")

    ctx = {
        "store_identity": (exp or {}).get("store_identity"),
        "time_series_tail": (exp or {}).get("time_series_analysis_data", [])[-12:] if exp else [],
        "transformation": trans,
        "report": report_md,
    }
    prompt = (
        "ë‹¹ì‹ ì€ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "ìˆ«ìëŠ” í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ ëª…í™•íˆ, ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¶€ì¡±í•˜ë‹¤ê³  ë°í˜€ì£¼ì„¸ìš”.\n\n"
        f"### ì»¨í…ìŠ¤íŠ¸\n{json.dumps(ctx, ensure_ascii=False, indent=2)}\n\n"
        f"### ì‚¬ìš©ìì˜ ì§ˆë¬¸\n{user_question}\n\n"
        "### ë‹µë³€"
    )
    try:
        resp = model.generate_content(prompt)
        return resp.text.strip()
    except Exception as e:
        return f"ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# í›„ë³´ í‘œì‹œ í¬ë§·
def _id_of(cand: dict):
    return cand.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸")

def _fmt_cand_with_id(c: dict) -> str:
    id_val = _id_of(c) or "UNKNOWN_ID"
    area = c.get("ìƒê¶Œ", "ìƒê¶Œ ì •ë³´ ì—†ìŒ")
    name = c.get("ê°€ë§¹ì ëª…", "?")
    industry = c.get("ì—…ì¢…", "?")
    return f"[{id_val}] {name} / {industry} / {area}"

def _set_step(step: str):
    st.session_state.pipeline["step"] = step

# =========================
# 5) ì‚¬ì´ë“œë°”
# =========================
with st.sidebar:
    logo = ASSETS / "shc_ci_basic_00.png"
    if logo.exists():
        st.image(load_image("shc_ci_basic_00.png"), use_container_width=True)
    st.markdown("<p style='text-align: center;'>2025 Big Contest â€¢ AI DATA í™œìš©</p>", unsafe_allow_html=True)
    st.button("Clear Chat History", on_click=clear_chat_history)

    # ğŸ”´ ì§„í–‰ ì¤‘ ì¤‘ì§€ ë²„íŠ¼ (ì¬ì‹¤í–‰ë˜ì–´ë„ 'running' ìœ ì§€)
    col_stop = st.container()
    if st.session_state.pipeline["running"]:
        if col_stop.button("ë¶„ì„ ì¤‘ë‹¨", type="secondary"):
            st.session_state.pipeline["cancel_requested"] = True
        
# --- íˆìŠ¤í† ë¦¬ ë Œë” ---
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.write(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("assistant"):
            st.write(msg.content)

# --- ì‹œê³„ì—´ ë Œë” ---
@st.cache_data(show_spinner=False)
def _prepare_ts(ts_list: list) -> pd.DataFrame:
    df = pd.DataFrame(ts_list)
    if "ê¸°ì¤€ë…„ì›”" in df.columns:
        try:
            df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(df["ê¸°ì¤€ë…„ì›”"])
        except Exception:
            pass
        df = df.sort_values("ê¸°ì¤€ë…„ì›”")
    return df.reset_index(drop=True)

def render_timeseries(exploration: dict, merchant_id: str | None = None):
    ts = exploration.get("time_series_analysis_data") or []
    if not ts:
        st.info("ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = _prepare_ts(ts)
    st.subheader("ğŸ“Š ìµœê·¼ 12ê°œì›” ë°ì´í„° ìš”ì•½")
    st.dataframe(df.tail(12), use_container_width=True, key=f"ts_tail_{merchant_id or 'na'}")

def create_kpi_charts(kpi_list: list, timeseries_df: pd.DataFrame) -> list:
    """
    AIê°€ ì„ ì •í•œ KPI ë¦¬ìŠ¤íŠ¸ì™€ ì‹œê³„ì—´ ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ Plotly ê·¸ë˜í”„ ìƒì„±
    PLOTTING_NUMERIC_COLSì— í¬í•¨ëœ ë³€ìˆ˜ë§Œ ê·¸ë˜í”„ë¡œ
    """
    charts = []
    
    potential_cols = [col for col in timeseries_df.columns if any(col in kpi_item for kpi_item in kpi_list)]
    valid_kpi_columns = [col for col in potential_cols if col in PLOTTING_NUMERIC_COLS]

    for kpi_col in sorted(set(valid_kpi_columns))[:4]:
        fig = px.line(
            timeseries_df,
            x='ê¸°ì¤€ë…„ì›”',
            y=kpi_col,
            title=f'<b>{kpi_col} ì¶”ì´</b>',
            markers=True,
            labels={'ê¸°ì¤€ë…„ì›”': 'ì›”', kpi_col: 'ê°’'}
        )
        fig.update_layout(
            title_font_size=20,
            title_x=0.5,
            xaxis_title_font_size=16,
            yaxis_title_font_size=16,
            legend_title_text=''
        )
        charts.append(fig)
            
    return charts

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ---
# âœ… ë°ì´í„° ë¡œë”© ë° Agent ì´ˆê¸°í™” (ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ)
@st.cache_resource
def load_data_and_init_agent():
    # ë°ì´í„° ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.
    try:
        # ì´ ë¶€ë¶„ì€ mcp_serverì˜ _load_df ë¡œì§ê³¼ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        file_path = "./data/labeling_no_preprocessing.csv"
        df = pd.read_csv(file_path)
        df.columns = df.columns.str.strip()
        df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(df["ê¸°ì¤€ë…„ì›”"].astype(str).str.zfill(6), format="%Y%m")
        agent = InteractiveParallelAgent(df, model)
        return agent, df
    except FileNotFoundError:
        st.error(f"ë°ì´í„° íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        st.stop()

agent, df = load_data_and_init_agent()

# âœ… mcp_serverë¥¼ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ , Agentê°€ ì§ì ‘ ê²€ìƒ‰í•˜ë„ë¡ ë³€ê²½
def search_merchant_local(store_name: str) -> Dict[str, Any]:
    search_term = (store_name or "").replace("*", "")
    hits = df[df["ê°€ë§¹ì ëª…"].astype(str).str.contains(search_term, case=False, na=False)]
    if hits.empty:
        return {"status": "error", "message": f"'{search_term}'ë¡œ ê²€ìƒ‰ëœ ê°€ë§¹ì ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    cand_cols = ["ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸", "ê°€ë§¹ì ëª…", "ì—…ì¢…", "ìƒê¶Œ", "ê°€ë§¹ì ì£¼ì†Œ"]
    cands = hits[cand_cols].drop_duplicates().to_dict(orient="records")

    if len(cands) == 1:
        return {"status": "single_candidate", "candidate": cands[0]}
    return {"status": "clarification_needed", "candidates": cands}


def run_full_pipeline_resumable(user_text: str | None = None):
    P = st.session_state.pipeline
    
    if not P.get("running"):
        if user_text is None: return
        P.update({"running": True, "step": "transform", "user_query": user_text})

    if P["step"] == "transform":
        if not st.session_state.get("log_entries"):
            logger.log("ğŸ¤” 1ë‹¨ê³„: ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ëŠ” ì¤‘...")
        trans = agent.transform(P["user_query"])
        if not trans or "error" in trans or not trans.get("target"):
            user_err("ìš”ì²­ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê°€ë§¹ì  ì´ë¦„ì„ ë” ëª…í™•í•˜ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            P["running"] = False; _set_step("idle"); return
        P["trans"] = trans
        logger.log("âœ… 1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì™„ë£Œ", details=trans, expander_label="ì‚¬ìš©ì ìš”ì²­ ë‚´ì—­ ë³´ê¸°", update=True)
        _set_step("search"); st.rerun()

    if P["step"] == "search":
        if len(st.session_state.log_entries) < 2:
             logger.log(f"ğŸ” 2ë‹¨ê³„: '{P['trans']['target']}' ê°€ë§¹ì  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘...")
        resp = search_merchant_local(P['trans']['target'])
        if resp["status"] == "error":
            user_err(resp["message"]); P["running"] = False; _set_step("idle"); return
        if resp["status"] == "clarification_needed":
            st.session_state.candidates = resp["candidates"]; st.session_state.awaiting_candidate = True
            logger.log("âš ï¸ 2ë‹¨ê³„: í›„ë³´ ê°€ë§¹ì  ë°œê²¬. ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", details=resp, expander_label="í›„ë³´ ê°€ë§¹ì  ëª©ë¡ ë³´ê¸°", update=True)
            return
        if resp["status"] == "single_candidate":
            P["merchant_id"] = resp["candidate"]["ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"]
            logger.log(f"âœ… 2ë‹¨ê³„: '{resp['candidate']['ê°€ë§¹ì ëª…']}' ê°€ë§¹ì  ì •ë³´ í™•ì¸ ì™„ë£Œ", details=resp, expander_label="í™•ì¸ëœ ê°€ë§¹ì  ì •ë³´ ë³´ê¸°", update=True)
            _set_step("analysis"); st.rerun()

    if P["step"] == "analysis":
        agent.set_target_store(P["merchant_id"])
        
        # ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•´ ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        P["timeseries_data"] = agent.context.get('target_store_timeseries')

        exploration = agent.compress_store_data()
        if "error" in exploration:
            user_err(f"ë°ì´í„° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {exploration['error']}"); P["running"] = False; _set_step("idle"); return
        P["exploration"] = exploration
        logger.log("âœ”ï¸ 3-1: ê°€ë§¹ì  ë°ì´í„° ìš”ì•½ ì™„ë£Œ", details=exploration, expander_label="ë°ì´í„° ìš”ì•½ ê²°ê³¼ ë³´ê¸°")

        pd_out = agent.define_problem()
        if "error" in pd_out or not pd_out.get("problem_statement"):
            user_err(f"AIê°€ ë¬¸ì œì ì„ ì •ì˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", details=pd_out); P["running"] = False; _set_step("idle"); return
        P["pd_out"] = pd_out
        logger.log("âœ”ï¸ 3-2: í•µì‹¬ ë¬¸ì œì  ì •ì˜ ì™„ë£Œ", details=pd_out, expander_label="ì •ì˜ëœ ë¬¸ì œì  ë³´ê¸°")

        report_md = agent.generate_report()
        if not report_md or "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨" in report_md:
            user_err("ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        P["report_md"] = report_md
        logger.log("âœ”ï¸ 3-3: ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ ìƒì„± ì™„ë£Œ")
        
        _set_step("done"); st.rerun()

    if P["step"] == "done":
        P["running"] = False
        _set_step("idle")
        st.rerun()

# --- ì…ë ¥ ë° ì œì–´ ë¡œì§ ---

# 1. Logger ê°ì²´ ìƒì„±
logger = StreamlitLogHandler()

# 2. ìƒˆë¡œìš´ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_query = st.chat_input("ê°€ë§¹ì  ì´ë¦„ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”.")
if user_query:
    st.session_state.messages.append(HumanMessage(content=user_query))
    with st.chat_message("user"):
        st.write(user_query)

    # ë³´ê³ ì„œ ìœ ë¬´ì— ë”°ë¼ "ì¶”ê°€ ì§ˆë¬¸"ê³¼ "ìƒˆ ë¶„ì„" êµ¬ë¶„
    if st.session_state.pipeline.get("report_md"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            reply = handle_followup(user_query)
        st.session_state.messages.append(AIMessage(content=reply))
        st.rerun()
    else:
        # ìƒˆë¡œìš´ ë¶„ì„ì´ë¯€ë¡œ ì´ì „ ë¡œê·¸ì™€ ê²°ê³¼ ì´ˆê¸°í™”
        st.session_state.log_entries = []
        st.session_state.pipeline.update({
            "report_md": None, "exploration": None, "pd_out": None, "trans": None
        })
        run_full_pipeline_resumable(user_text=user_query)

# 3. UI ë Œë”ë§ ë° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì„ ì œì–´
P = st.session_state.pipeline

# 3-1. íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ ì¤‘ì¼ ë•Œì˜ UI ë° ë¡œì§
if P.get("running"):
    with st.spinner("â³ AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # ìŠ¤í”¼ë„ˆ ë‚´ë¶€ì—ì„œ ë¡œê·¸ë¥¼ ê·¸ë ¤ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ì²˜ëŸ¼ ë³´ì´ë„ë¡
        st.write("---")
        st.subheader("ğŸ¤– AI ì—ì´ì „íŠ¸ ë¶„ì„ ê³¼ì •")
        log_container = st.container(border=True)
        logger.render(log_container)

        if not st.session_state.awaiting_candidate:
            run_full_pipeline_resumable()

# 3-2. íŒŒì´í”„ë¼ì¸ì´ ëë‚¬ì„ ë•Œì˜ UI (ë˜ëŠ” ì¶”ê°€ ì§ˆë¬¸ ì‹œ)
else:
    # ë¶„ì„ ë¡œê·¸ í‘œì‹œ
    if st.session_state.get("log_entries"):
        st.write("---")
        st.subheader("ğŸ¤– AI ì—ì´ì „íŠ¸ ë¶„ì„ ê³¼ì •")
        log_container = st.container(border=True)
        logger.render(log_container)

    # ìµœì¢… ë³´ê³ ì„œ ë° KPI ê·¸ë˜í”„ í‘œì‹œ
    if P.get("report_md"):
        # KPI ê·¸ë˜í”„ í‘œì‹œ
        if P.get("pd_out") and P.get("timeseries_data") is not None:
            kpi_list = P["pd_out"].get("kpis", [])
            if kpi_list:
                st.subheader("ğŸ“Š ì£¼ìš” KPI ì‹œê³„ì—´ ì¶”ì´")
                charts = create_kpi_charts(kpi_list, P["timeseries_data"])
                if charts:
                    for chart in charts:
                        st.plotly_chart(chart, use_container_width=True)
                else:
                    st.info("ì„ ì •ëœ KPIì— ëŒ€í•œ ì‹œê³„ì—´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœì¢… ë³´ê³ ì„œ í‘œì‹œ
        st.subheader("ğŸ“˜ ìµœì¢… ë³´ê³ ì„œ (ìƒì„¸)")
        st.markdown(P["report_md"], unsafe_allow_html=True)

# 4. í›„ë³´ ê°€ë§¹ì  ì„ íƒ UI ì²˜ë¦¬
if st.session_state.awaiting_candidate and st.session_state.candidates:
    st.info("ì´ë¦„ì´ ìœ ì‚¬í•œ ê°€ë§¹ì ì´ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    cands = st.session_state.candidates
    idx = st.radio("ê°€ë§¹ì ì„ ì„ íƒí•˜ì„¸ìš”", options=range(len(cands)), format_func=lambda i: _fmt_cand_with_id(cands[i]), label_visibility="collapsed")
    
    if st.button("ì„ íƒ í™•ì •", key="cand_confirm", type="primary"):
        sel = cands[idx]
        st.session_state.awaiting_candidate = False
        st.session_state.pipeline["merchant_id"] = str(sel.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"))
        _set_step("analysis")
        st.rerun()
    st.stop()

# 5. ë¶„ì„ ì¤‘ë‹¨ ìš”ì²­ ì²˜ë¦¬
if st.session_state.pipeline.get("running") and st.session_state.pipeline.get("cancel_requested"):
    st.session_state.pipeline.update({"running": False, "step": "idle", "cancel_requested": False})
    st.info("â¹ï¸ ë¶„ì„ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
    st.rerun()