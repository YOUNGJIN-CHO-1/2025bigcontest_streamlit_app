import os
import sys
import json
import time
import shutil
import asyncio
import base64
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re, difflib
import matplotlib.pyplot as plt
import html
import logging

from typing import Any, Dict, List
from pathlib import Path
from PIL import Image

from contextlib import AsyncExitStack

from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# =========================
# 0) ê³µí†µ ì„¤ì •
# =========================
ASSETS = Path("assets")
st.set_page_config(
    page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬",
    layout="wide",
    page_icon="ğŸ“ˆ",
)

# âœ¨ ì œëª©/ë ˆì´ì•„ì›ƒ CSS: í•œêµ­ì–´ ì¤„ë°”ê¿ˆ, ë°˜ì‘í˜• í¬ê¸°, ì˜ë¦¼ ë°©ì§€
st.markdown("""
<style>
.block-container { padding-top: .8rem; padding-bottom: 1.2rem; max-width: 100%; }
.element-container, .stDataFrame { width: 100% !important; }

/* íƒ€ì´í‹€: í•œê¸€ ì¤„ë°”ê¿ˆ + ë°˜ì‘í˜• + ì˜ë¦¼ ë°©ì§€ */
.app-title{
  font-weight: 800;
  line-height: 1.25;
  font-size: clamp(22px, 2.2vw + 14px, 40px);
  margin: .2rem 0 1.0rem 0;
  white-space: normal;
  word-break: keep-all;
  overflow-wrap: anywhere;
}
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="app-title">ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ ğŸ”‘</h1>', unsafe_allow_html=True)
# ë¡œê¹…: í™”ë©´ ëŒ€ì‹  ì½˜ì†”ë¡œë§Œ
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

def user_warn(msg: str):
    st.warning(msg, icon="âš ï¸")

def user_err(msg: str):
    st.error(msg, icon="ğŸ›‘")

# (í˜„ì¬ ì½”ë“œ ìœ ì§€â€”í•˜ë“œì½”ë“œ)
GOOGLE_API_KEY = "AIzaSyB8R3nurDOohfAvKXSgBUVRkoliXtfnTKo"
genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel("gemini-2.5-flash")
async_model = genai.GenerativeModel("gemini-2.5-flash")

def _llm_json_sync(gen_model, prompt: str) -> Dict[str, Any]:
    try:
        time.sleep(0.6)  # ê³¼í˜¸ì¶œ ë°©ì§€ìš© ì§§ì€ ì§€ì—°
        resp = gen_model.generate_content(
            prompt,
            generation_config={"response_mime_type": "application/json"}
        )
        return json.loads(resp.text)
    except Exception as e:
        return {"error": f"LLM ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"}

# ---------------------------
# ì—ì´ì „íŠ¸(ì¶”ë¡ /ë¦¬í¬íŒ…) ë¡œì§
# ---------------------------
class InteractiveParallelAgent:
    """
    - ë³€í™˜ â†’ ë¬¸ì œì •ì˜ â†’ ì „ëµ â†’ (í†µí•©)ìµœì¢… ë³´ê³ ì„œ
    - ë³´ê³ ì„œëŠ” í†µí•© í”„ë¡¬í”„íŠ¸(_GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT) ì‚¬ìš©
    - ì™¸ë¶€(run_full_pipeline)ì—ì„œ xploration(ì‹œê³„ì—´) ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
    """
        
    # 1) ë³€í™˜
    _TRANSFORM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë„¤ ê°€ì§€ JSON í•­ëª©ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- "target": ì‚¬ìš©ìê°€ ì „ëµì„ ìš”ì²­í•œ ê°€ë§¹ì ëª…ì€ ë¬´ì—‡ì¸ê°€ìš”? (ê°€ë§¹ì  ì´ë¦„. ì˜ˆ: 'ì„±ìš°**')
- "challenge": ì‚¬ìš©ìê°€ ì§ë©´í•œ ì–´ë ¤ì›€ì´ë‚˜ í•´ê²°í•´ì•¼ í•  í•µì‹¬ ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
- "objective": ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ëª©í‘œì€ ë¬´ì—‡ì¸ê°€ìš”?
- "solution_direction": ì œì•ˆë  ìˆ˜ ìˆëŠ” í•´ê²°ì±…ì˜ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
"""

    # 2) ë¬¸ì œ ì •ì˜
    _DEFINE_PROBLEM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ì§€ì¹¨ì— ë”°ë¼ 'ë¬¸ì œ ì •ì˜'ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”. :
```
1. ì œê³µëœ ê°€ë§¹ì ì˜ ì‹œê³„ì—´ ë°ì´í„°ì™€ ìµœì´ˆ ì‚¬ìš©ì ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ, ë¬¸ì œ ìƒí™©ê³¼ í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.
íŠ¹íˆ, ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ë°ì´í„°ì˜ ë³€í™”ì™€ ì¶”ì„¸(ì˜ˆ: ì¬ë°©ë¬¸ìœ¨ í•˜ë½, íŠ¹ì • ê³ ê°ì¸µ ë¹„ì¤‘ ê°ì†Œ ë“±)ë¥¼ ëª…í™•í•˜ê²Œ íŒŒì•…í•˜ê³ , ì´ë¥¼ ë¬¸ì œ ì •ì˜ì— ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.
2. ìµœê·¼ì˜ ë³€í™”ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ë¬¸ì œë¥¼ ì •ì˜í•˜ì„¸ìš”.
3. ì œê³µëœ ë°ì´í„°ëŠ” ìƒëŒ€ì ì´ê³  ë¹„ìœ¨ì„ í‘œí˜„í•˜ëŠ” ìˆ˜ì¹˜ì„ì„ ìœ ì˜í•˜ì„¸ìš”.
```

ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. :
```
- ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„,ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„,ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„,ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„,ê°ë‹¨ê°€ êµ¬ê°„ : 6ê°œ êµ¬ê°„ìœ¼ë¡œ (10%ì´í•˜, 10-25%, 25-50%, 50-75%, 75-90%, 90%ì´ˆê³¼) 0%ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„.
- ì·¨ì†Œìœ¨ êµ¬ê°„ : 1êµ¬ê°„ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ìƒìœ„. (ì·¨ì†Œìœ¨ ë‚®ìŒ)
- ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨ : ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê¸ˆì•¡ í‰ê·  ëŒ€ë¹„ í•´ë‹¹ ê°€ë§¹ì  ë§¤ì¶œ ê¸ˆì•¡ ë¹„ìœ¨ (í‰ê· ê³¼ ë™ì¼ : 100%)
- ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨ : ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê±´ìˆ˜ í‰ê·  ëŒ€ë¹„ í•´ë‹¹ ê°€ë§¹ì  ë§¤ì¶œ ê±´ìˆ˜ ë¹„ìœ¨ (í‰ê· ê³¼ ë™ì¼ : 100%)
- ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨ : ì—…ì¢… ë‚´ ìˆœìœ„ / ì—…ì¢… ë‚´ ì „ì²´ ê°€ë§¹ì  * 100 (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„)
- ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨ : ìƒê¶Œ ë‚´ ìˆœìœ„ / ìƒê¶Œ ë‚´ ì „ì²´ ê°€ë§¹ì  * 100 (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„)
```

ê²°ê³¼ëŠ” ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
```
- "problem_statement": ì‹œê³„ì—´ ë³€í™”ì— ê¸°ë°˜í•œ ë¬¸ì œ ì •ì˜
- "kpis": ì¸¡ì • ê°€ëŠ¥í•œ í•µì‹¬ ì§€í‘œ ëª©ë¡ (ë¬¸ìì—´ ëª©ë¡ìœ¼ë¡œ ì œê³µ)
```
"""

    # 4) ì „ëµ ì œì•ˆ
    _GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 'ë¬¸ì œ ì •ì˜'ì™€ ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬, ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ë³´ê³ ì„œëŠ” ë‹¤ìŒì˜ ëª…í™•í•œ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. :
```
1. ì„œë¡ : ë¬¸ì œ ë°°ê²½ ë° ë¶„ì„ ëª©ì 
2. ë³¸ë¡ :
   - ë°ì´í„° ê¸°ë°˜ í˜„í™© ë¶„ì„: ë°ì´í„°ì˜ ì‹œê³„ì—´ì  ì¶”ì„¸(ì˜ˆ: ìµœê·¼ 3ê°œì›”ê°„ ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘ ê°ì†Œ)ë¥¼ ëª…í™•íˆ ì–¸ê¸‰í•˜ë©° í˜„ì¬ ìƒí™©ì„ ë¶„ì„
   - ë¬¸ì œ ì •ì˜: ë¶„ì„ì„ í†µí•´ ë„ì¶œëœ ê°€ì¥ ì‹œê¸‰í•˜ê³  ì¤‘ìš”í•œ í•µì‹¬ ë¬¸ì œ ì •ì˜
   - í•´ê²° ì „ëµ ì œì•ˆ: ì •ì˜ëœ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ ë° ì„¸ë¶€ ì‹¤í–‰ ë°©ì•ˆì„ ì œì‹œ
   - ê·¼ê±° ì œì‹œ: ì œì•ˆí•œ ì „ëµì´ íš¨ê³¼ì ì¸ ì´ìœ ë¥¼ ë°ì´í„° ì¶”ì„¸ì™€ ì—°ê²°í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…
3. ê²°ë¡ : ì œì•ˆëœ ì „ëµ ì‹¤í–‰ ì‹œ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ ì œì‹œ
```

**ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥ í˜•ì‹**
    - ë°˜ë“œì‹œ "report"ë¼ëŠ” ë‹¨ì¼ í‚¤ë¥¼ ê°€ì§„ JSON í˜•ì‹ì´ì–´ì•¼ í•¨
    - ê°’ì—ëŠ” ì „ì²´ ë³´ê³ ì„œ ë‚´ìš©ì´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í¬í•¨ë˜ì–´ì•¼ í•¨
    - ì˜ˆì‹œ: {"report": "## ìµœì¢… ë³´ê³ ì„œ\\n\\n### 1. ê°œìš”\\n...\\n### 2. í˜„í™© ë¶„ì„ ë° ë¬¸ì œ ì •ì˜\\nìµœê·¼ 3ê°œì›”ê°„ ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤...\\n### 3. í•´ê²° ì „ëµ\\n...\\n"}
"""

    def __init__(self, sync_model):
        self.model = sync_model
        self.context: Dict[str, Any] = {}

    def transform(self, initial_input: str):
        user = f"ğŸ”¹ ì‚¬ìš©ì ì…ë ¥: {initial_input}"
        full = f"{self._TRANSFORM_SYSTEM_PROMPT}\n\n{user}"
        self.context["transformation"] = _llm_json_sync(self.model, full)
        return self.context["transformation"]

    def define_problem(self, exploration: Dict[str, Any], transformation: Dict[str, Any]):
        ts_tail = (exploration.get("time_series_analysis_data") or [])[-12:]
        slim_ctx = {
            "ìµœì´ˆìš”ì²­": transformation,
            "ì‹œê³„ì—´_tail": ts_tail,
            "store_identity": exploration.get("store_identity")
        }
        user = "ğŸ”¹ ë¬¸ë§¥:\n" + json.dumps(slim_ctx, ensure_ascii=False, indent=2)
        full = f"{self._DEFINE_PROBLEM_SYSTEM_PROMPT}\n\n{user}"
        self.context["problem_definition"] = _llm_json_sync(self.model, full)
        return self.context["problem_definition"]


    def _compact_context(self, max_rows: int = 12) -> Dict[str, Any]:
        exp = self.context.get("exploration") or {}
        ts = (exp.get("time_series_analysis_data") or [])[-max_rows:]
        return {
            "transformation": self.context.get("transformation"),
            "problem_definition": self.context.get("problem_definition"),
            "exploration_summary": {
                "store_identity": exp.get("store_identity"),
                "time_series_tail": ts,
                "tail_note": f"ìµœê·¼ {min(max_rows, len(ts))}ê°œì›”ë§Œ í¬í•¨",
            },
        }

    def generate_strategy_and_report(self) -> str:
        compact = self._compact_context(max_rows=12)
        user = "ğŸ”¹ ì „ì²´ ì»¨í…ìŠ¤íŠ¸:\n" + json.dumps(compact, ensure_ascii=False, indent=2)
        out = _llm_json_sync(self.model, f"{self._GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT}\n\n{user}")

        if isinstance(out, dict) and out.get("report"):
            return out["report"]

        raw = out.get("raw") if isinstance(out, dict) else None
        if raw:
            return raw if raw.strip().startswith("#") else f"# ìµœì¢… ë³´ê³ ì„œ(ìë™ ë³µêµ¬)\n\n{raw}"

        return "ë³´ê³ ì„œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

class _McpSingleton:
    def __init__(self):
        self.inited = False
        self.stack: AsyncExitStack | None = None
        self.session = None
        self.tool_map = None

    async def _ainit(self):
        if self.inited:
            return

        server_path = (Path(__file__).parent / "mcp_server_v3.py").resolve()
        uv_path = shutil.which("uv")
        command = uv_path if uv_path else sys.executable
        cmd_args = ["run", str(server_path)] if uv_path else ["-u", str(server_path)]
        env = os.environ.copy()
        env.setdefault("PYTHONUNBUFFERED", "1")
        env.setdefault("PYTHONIOENCODING", "utf-8")

        server_params = StdioServerParameters(command=command, args=cmd_args, env=env)

        # âš ï¸ í•µì‹¬: async context managerë¥¼ AsyncExitStackìœ¼ë¡œ ì§„ì…
        self.stack = AsyncExitStack()
        await self.stack.__aenter__()

        # stdio_clientëŠ” async generator context managerì…ë‹ˆë‹¤.
        read, write = await self.stack.enter_async_context(stdio_client(server_params))
        # ClientSessionë„ async context managerì…ë‹ˆë‹¤.
        self.session = await self.stack.enter_async_context(ClientSession(read, write))

        await self.session.initialize()
        tools = await load_mcp_tools(self.session)
        self.tool_map = {t.name: t for t in tools}
        self.inited = True

    async def aclose(self):
        if self.stack is not None:
            await self.stack.aclose()
        self.inited = False
        self.stack = None
        self.session = None
        self.tool_map = None

    async def ainvoke(self, tool_name: str, args: dict):
        if not self.inited:
            await self._ainit()
        if tool_name not in self.tool_map:
            return {"error": f"íˆ´ '{tool_name}' ì—†ìŒ", "available": list(self.tool_map.keys())}
        return await self.tool_map[tool_name].ainvoke(args)

_mcp_singleton = _McpSingleton()

# ---------------------------
# MCP í˜¸ì¶œ ìœ í‹¸
# ---------------------------
@st.cache_resource(show_spinner=False)
def _get_mcp_client():
    # Streamlitì€ ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ ì—¬ê¸°ì„œ event loopë¥¼ ë³´ì¥í•´ ì¤ë‹ˆë‹¤.
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(_mcp_singleton._ainit())
    return _mcp_singleton

def _normalize_mcp_result(result):
    """
    MCP íˆ´ ë°˜í™˜ì„ ì•ˆì „í•˜ê²Œ dictë¡œ í‘œì¤€í™”.
    - JSON stringì´ë©´ íŒŒì‹±
    - listë©´ {"items": [...]}
    - None/ê¸°íƒ€ëŠ” {"raw": ...}ë¡œ ë˜í•‘
    """
    if result is None:
        return {"status": "error", "message": "empty result"}
    if isinstance(result, dict):
        return result
    if isinstance(result, str):
        try:
            obj = json.loads(result)
            if isinstance(obj, dict):
                return obj
            else:
                return {"raw": obj}
        except Exception:
            return {"raw": result}
    if isinstance(result, list):
        return {"items": result}
    # ê¸°íƒ€ íƒ€ì…
    return {"raw": result}

def mcp_call(tool_name: str, args: dict) -> dict:
    client = _get_mcp_client()
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    try:
        res = loop.run_until_complete(client.ainvoke(tool_name, args))
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        res = loop.run_until_complete(client.ainvoke(tool_name, args))
    except Exception as e:
        # ë„¤íŠ¸ì›Œí¬/í”„ë¡œì„¸ìŠ¤ ì˜ˆì™¸ë„ dictë¡œ í†µì¼
        return {"status": "error", "message": f"MCP invoke exception: {e.__class__.__name__}: {e}"}

    return _normalize_mcp_result(res)

# ---------------------------
# Streamlit UI
# ---------------------------
system_prompt = (
    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , "
    "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•ê³¼ ì±„ë„, ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. "
    "ê²°ê³¼ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ, ë¶„ì„ ê²°ê³¼ì—ëŠ” ê°€ëŠ¥í•œ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
greeting = "ë§ˆì¼€íŒ…ì´ í•„ìš”í•œ ê°€ë§¹ì ì„ ì•Œë ¤ì£¼ì„¸ìš”."

# ===== ì„¸ì…˜ ìƒíƒœ ì•ˆì „ ì´ˆê¸°í™” =====
def ensure_state():
    ss = st.session_state
    # ëŒ€í™”/ì—ì´ì „íŠ¸
    if "messages" not in ss:
        ss.messages = [
            SystemMessage(content=system_prompt),
            AIMessage(content=greeting),
        ]
    if "agent" not in ss:
        ss.agent = InteractiveParallelAgent(sync_model=model)

    # ì¡°íšŒ/ì„ íƒ/ì‚°ì¶œë¬¼
    ss.setdefault("awaiting_candidate", False)
    ss.setdefault("candidates", [])
    ss.setdefault("exploration", None)
    ss.setdefault("transformation", None)
    ss.setdefault("selected_merchant_id", None)
    ss.setdefault("last_chart_b64", None)
    ss.setdefault("user_query", "")

    # íŒŒì´í”„ë¼ì¸(ì´ë¯¸ ë§Œë“œì…¨ì§€ë§Œ, í˜¹ì‹œ ì—†ì„ ë•Œë¥¼ ëŒ€ë¹„)
    if "pipeline" not in ss:
        ss.pipeline = {
            "running": False,
            "cancel_requested": False,
            "step": "idle",
            "merchant_id": None,
            "user_query": "",
            "trans": None,
            "candidates": [],
            "exploration": None,
            "pd_out": None,
            "chart_b64": None,
            "used_kpis": None,
            "report_md": None,
        }

# âœ… ë°˜ë“œì‹œ ì‚¬ì´ë“œë°”/í™”ë©´ ë Œë” ì „ì— í˜¸ì¶œ
ensure_state()

@st.cache_data
def load_image(name: str):
    return Image.open(ASSETS / name)

def clear_chat_history():
    st.session_state.messages = [
        SystemMessage(content=system_prompt),
        AIMessage(content=greeting),
    ]
    st.session_state.awaiting_candidate = False
    st.session_state.candidates = []
    st.session_state.exploration = None
    st.session_state.transformation = None
    st.session_state.selected_merchant_id = None
    st.session_state.last_chart_b64 = None
    st.session_state.user_query = ""

def handle_followup(user_question: str) -> str:
    """
    í˜„ì¬ ì„¸ì…˜ì˜ exploration / transformation / reportë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•´
    ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. ìƒˆ íŒŒì´í”„ë¼ì¸ì„ ëŒë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    exp = st.session_state.pipeline.get("exploration")
    trans = st.session_state.pipeline.get("trans")
    report_md = st.session_state.pipeline.get("report_md")

    ctx = {
        "store_identity": (exp or {}).get("store_identity"),
        "time_series_tail": (exp or {}).get("time_series_analysis_data")[-12:] if exp else [],
        "transformation": trans,
        "report": report_md,
    }
    prompt = (
        "ë‹¹ì‹ ì€ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "ìˆ«ìëŠ” í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ ëª…í™•íˆ, ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¶€ì¡±í•˜ë‹¤ê³  ë°í˜€ì£¼ì„¸ìš”.\n\n"
        f"### ì»¨í…ìŠ¤íŠ¸\n{json.dumps(ctx, ensure_ascii=False, indent=2)}\n\n"
        f"### ì‚¬ìš©ìì˜ ì§ˆë¬¸\n{user_question}\n\n"
        "### ë‹µë³€"
    )
    try:
        resp = model.generate_content(prompt)
        return resp.text.strip()
    except Exception as e:
        return f"ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# í›„ë³´ í‘œì‹œ í¬ë§·
def _id_of(cand: dict):
    return cand.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸")

def _fmt_cand_with_id(c: dict) -> str:
    id_val = _id_of(c) or "UNKNOWN_ID"
    area = c.get("ìƒê¶Œ", None)
    area_txt = "(ìƒê¶Œ ì—†ìŒ)" if (area is None) else str(area)
    name = c.get("ê°€ë§¹ì ëª…", "?")
    industry = c.get("ì—…ì¢…", "?")
    return f"[{id_val}] {name} / {industry} / {area_txt}"

# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ê¸°ì¡´ ë¸”ë¡ì— ì¶”ê°€) ===
if "pipeline" not in st.session_state:
    st.session_state.pipeline = {
        "running": False,            # íŒŒì´í”„ë¼ì¸ ë™ì‘ ì—¬ë¶€
        "cancel_requested": False,   # ì¤‘ì§€ ìš”ì²­ í”Œë˜ê·¸
        "step": "idle",              # í˜„ì¬ ë‹¨ê³„
        "merchant_id": None,
        "user_query": "",
        # ì‚°ì¶œë¬¼ ìºì‹œ(ì¬ì‹¤í–‰ ì‹œ ì´ì–´ê°€ê¸° ìœ„í•¨)
        "trans": None,
        "candidates": [],
        "exploration": None,
        "pd_out": None,
        "chart_b64": None,
        "used_kpis": None,
        "report_md": None,
    }

def _set_step(step: str):
    st.session_state.pipeline["step"] = step

def _cancel_requested() -> bool:
    return bool(st.session_state.pipeline.get("cancel_requested"))

def _abort_if_cancelled():
    if _cancel_requested():
        st.info("â¹ï¸ ì „ëµ ì œì•ˆì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
        st.session_state.pipeline["running"] = False
        _set_step("idle")
        raise st.stop()  # í˜„ì¬ ë Œë” ì¤‘ë‹¨ (ë‹¤ìŒ ì¬ì‹¤í–‰ ë•ŒëŠ” idle ìƒíƒœ)

# =========================
# 5) ì‚¬ì´ë“œë°”
# =========================
with st.sidebar:
    logo = ASSETS / "shc_ci_basic_00.png"
    if logo.exists():
        st.image(load_image("shc_ci_basic_00.png"), use_container_width=True)
    st.markdown("<p style='text-align: center;'>2025 Big Contest â€¢ AI DATA í™œìš©</p>", unsafe_allow_html=True)
    st.button("Clear Chat History", on_click=clear_chat_history)

    # ğŸ”´ ì§„í–‰ ì¤‘ ì¤‘ì§€ ë²„íŠ¼ (ì¬ì‹¤í–‰ë˜ì–´ë„ 'running' ìœ ì§€)
    col_stop = st.container()
    if st.session_state.pipeline["running"]:
        if col_stop.button("ì „ëµ ì œì•ˆ ì¢…ë£Œ", type="secondary"):
            st.session_state.pipeline["cancel_requested"] = True
        
# --- íˆìŠ¤í† ë¦¬ ë Œë” ---
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.write(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("assistant"):
            st.write(msg.content)

# --- KPI ì´ë¦„ ì •ê·œí™”/ë§¤í•‘ ìœ í‹¸ ---
def _normalize_kpi_name(name: str) -> str:
    if not name:
        return ""
    s = str(name).strip()
    s = re.sub(r"\s*\(.*?\)\s*$", "", s)  # ê´„í˜¸ ì„¤ëª… ì œê±°
    synonyms = {
        "ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨": "ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ê¸ˆì•¡ ë¹„ìœ¨",
        "ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê¸ˆì•¡ ë¹„ìœ¨": "ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ê¸ˆì•¡ ë¹„ìœ¨",
        "ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨": "ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ê±´ìˆ˜ ë¹„ìœ¨",
        "ë™ì¼ ì—…ì¢… ë§¤ì¶œ ê±´ìˆ˜ ë¹„ìœ¨": "ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ê±´ìˆ˜ ë¹„ìœ¨",
        "ë™ì¼ ìƒê¶Œ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨": "ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨",
        "ì·¨ì†Œìœ¨": "ì·¨ì†Œìœ¨ êµ¬ê°„",
    }
    s = synonyms.get(s, s)
    s = re.sub(r"\s+", " ", s)
    return s

def _map_kpis_to_available(requested: list[str], available: list[str]) -> list[str]:
    if not requested:
        return []
    req_norm = [_normalize_kpi_name(k) for k in requested if k]
    avail_norm = [re.sub(r"\s+", " ", str(a).strip()) for a in available]

    chosen = []
    def pick(c):
        if c and c not in chosen:
            chosen.append(c)

    # ì™„ì „ì¼ì¹˜
    for r in req_norm:
        for a in avail_norm:
            if r == a:
                pick(a)

    # ë¶€ë¶„ í¬í•¨
    for r in req_norm:
        if any(r == c for c in chosen):
            continue
        hits = [a for a in avail_norm if r in a or a in r]
        if hits:
            pick(hits[0])

    # ìœ ì‚¬ë„
    for r in req_norm:
        if any(r == c or r in c or c in r for c in chosen):
            continue
        near = difflib.get_close_matches(r, avail_norm, n=1, cutoff=0.5)
        if near:
            pick(near[0])

    return chosen[:5]


# --- ì‹œê³„ì—´ ë Œë” ---
@st.cache_data(show_spinner=False)
def _prepare_ts(ts_list: list) -> pd.DataFrame:
    df = pd.DataFrame(ts_list)
    if "ê¸°ì¤€ë…„ì›”" in df.columns:
        try:
            df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(df["ê¸°ì¤€ë…„ì›”"])
        except Exception:
            pass
        df = df.sort_values("ê¸°ì¤€ë…„ì›”")
    return df.reset_index(drop=True)

def render_timeseries(exploration: dict, merchant_id: str | None = None):
    ts = exploration.get("time_series_analysis_data") or []
    if not ts:
        st.info("ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = _prepare_ts(ts)

    # â‘¡ ì „ì²´
    st.subheader("â‘¡ ì‹œê³„ì—´ ì „ì²´(ìµœê·¼ 12ê°œì›”)")
    st.dataframe(df.tail(12), use_container_width=True, key=f"ts_tail_{merchant_id or 'na'}")

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ---
def run_full_pipeline_resumable(user_text: str | None = None, merchant_id: str | None = None):
    P = st.session_state.pipeline
    agent = st.session_state.agent

    # ìµœì´ˆ ì§„ì…: running=false â†’ true ì „í™˜
    if not P["running"]:
        if user_text is None:
            return
        P.update({
            "running": True,
            "cancel_requested": False,
            "step": "transform",
            "user_query": user_text,
            "merchant_id": merchant_id,
            "trans": None,
            "candidates": [],
            "exploration": None,
            "pd_out": None,
            "chart_b64": None,
            "used_kpis": None,
            "report_md": None,
        })

    # ---- 1) ë³€í™˜ ----
    if P["step"] == "transform":
        _abort_if_cancelled()
        trans = agent.transform(P["user_query"])
        st.session_state.transformation = trans
        P["trans"] = trans
        _set_step("search")
        st.toast("ë³€í™˜ ì™„ë£Œ", icon="âœ…")

    # ---- 2) ê°€ë§¹ì  ê²€ìƒ‰/ì„ íƒ ----
    if P["step"] == "search":
        _abort_if_cancelled()
        target = (P["trans"] or {}).get("target") or P["user_query"]
        resp = mcp_call("search_merchant", {"store_name": target})

        if not isinstance(resp, dict):
            logger.error(f"Unexpected MCP response type: {type(resp)} -> {resp}")
            resp = _normalize_mcp_result(resp)

        status = resp.get("status")
        if "error" in resp:
            user_err(f"ê°€ë§¹ì  ê²€ìƒ‰ ì˜¤ë¥˜: {resp.get('message', resp.get('error'))}")
            P["running"] = False; _set_step("idle"); return

        status = resp.get("status")
        if status == "clarification_needed":
            st.session_state.candidates = resp.get("candidates", [])
            P["candidates"] = resp.get("candidates", [])
            st.session_state.awaiting_candidate = True
            st.info("ğŸ” ìœ ì‚¬ ê°€ë§¹ì  ë‹¤ìˆ˜ â€” ì¢Œì¸¡ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„ íƒí•˜ì„¸ìš”.")
            # ì„ íƒë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì¬ì‹¤í–‰ ì‹œ ë‹¤ì‹œ ì—¬ê¸°ë¡œ ì˜´)
            return

        if status == "single_candidate":
            P["merchant_id"] = resp["candidate"]["ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"]
            _set_step("timeseries")
        else:
            user_err("ê²€ìƒ‰ ì‹¤íŒ¨ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” ìƒíƒœì…ë‹ˆë‹¤.")
            P["running"] = False; _set_step("idle"); return

    # ---- 3) ì‹œê³„ì—´ ì¡°íšŒ/í‘œì‹œ ----
    if P["step"] == "timeseries":
        _abort_if_cancelled()
        ts_resp = mcp_call("get_store_timeseries", {"merchant_id": str(P["merchant_id"])})
        if ts_resp.get("status") != "success":
            user_err(ts_resp.get("message", "ì‹œê³„ì—´ ì¡°íšŒ ì‹¤íŒ¨"))
            P["running"] = False; _set_step("idle"); return

        data = ts_resp["data"]
        exploration = {
            "store_identity": data.get("store_identity"),
            "time_series_analysis_data": data.get("time_series_data"),
        }
        st.session_state.exploration = exploration
        st.session_state.selected_merchant_id = P["merchant_id"]
        agent.context["exploration"] = exploration
        P["exploration"] = exploration

        render_timeseries(exploration, merchant_id=P["merchant_id"])
        _set_step("problem")

    # ---- 4) ë¬¸ì œ ì •ì˜ ----
    if P["step"] == "problem":
        _abort_if_cancelled()
        pd_out = agent.define_problem(P["exploration"], P["trans"])
        P["pd_out"] = pd_out
        _set_step("chart")

    # ---- 5) KPI ì°¨íŠ¸ ----
    if P["step"] == "chart":
        _abort_if_cancelled()
        kpis_from_llm = (P["pd_out"] or {}).get("kpis") or []
        chart_b64, used_kpis = None, None

        if P["merchant_id"] and kpis_from_llm:
            chart_resp = mcp_call("render_kpi_chart", {
                "merchant_id": str(P["merchant_id"]),
                "kpi_keys": kpis_from_llm,
                "title": f"KPI Trend â€“ {P['exploration']['store_identity'].get('name')}",
                "normalize_0_1": False,
            })
            if chart_resp.get("status") != "success":
                avail = chart_resp.get("available_kpis", []) or []
                # ë§¤í•‘ ì¬ì‹œë„
                mapped = _map_kpis_to_available(kpis_from_llm, avail)
                if mapped:
                    chart_resp = mcp_call("render_kpi_chart", {
                        "merchant_id": str(P["merchant_id"]),
                        "kpi_keys": mapped,
                        "title": f"KPI Trend â€“ {P['exploration']['store_identity'].get('name')}",
                        "normalize_0_1": False,
                    })

            if chart_resp.get("status") == "success":
                chart_b64 = (chart_resp.get("image") or {}).get("base64")
                used_kpis = chart_resp.get("used_kpis", [])
                st.session_state.last_chart_b64 = chart_b64

        P["chart_b64"], P["used_kpis"] = chart_b64, used_kpis
        _set_step("report")

    # ---- 6) ìµœì¢… ë³´ê³ ì„œ ----
    if P["step"] == "report":
        _abort_if_cancelled()
        report_md = agent.generate_strategy_and_report()
        if not report_md or "ë³´ê³ ì„œ ìƒì„±ì— ì‹¤íŒ¨" in report_md:
            si = (P["exploration"] or {}).get("store_identity", {})
            report_md = "\n".join([
                f"# {si.get('name','ê°€ë§¹ì ')} ì»¨ì„¤íŒ… ìš”ì•½",
                "## ë¬¸ì œ ì •ì˜", "```json",
                json.dumps(P["pd_out"], ensure_ascii=False, indent=2), "```",
                "## ì „ëµ ì œì•ˆ", "- ì§€ì—­ íƒ€ê²Ÿ ë©”ì‹œì§€ ê°•í™”",
                "- ì‹ ê·œ/ì¬ë°©ë¬¸ ìº í˜ì¸ ë™ì‹œ ìš´ì˜",
                "- ìƒê¶Œ ë‚´ ê²½ìŸ ëŒ€ë¹„ í”„ë¡œëª¨ì…˜ ì°¨ë³„í™”",
                "## ê¸°ëŒ€ íš¨ê³¼", "- ë§¤ì¶œ/ë°©ë¬¸ ì§€í‘œ ê°œì„  ê¸°ëŒ€", "- ì‹ ê·œ ìœ ì… ë° ì¬ë°©ë¬¸ í™•ëŒ€",
            ])
        P["report_md"] = report_md
        _set_step("done")

    # ---- 7) ì¶œë ¥ ----
    if P["step"] == "done":
        si = (P["exploration"] or {}).get("store_identity") or {}
        # â‘  ì±„íŒ…ì—” ìš”ì•½ë§Œ
        brief = []
        brief.append(f"**ê°€ë§¹ì :** {si.get('name')} / {si.get('industry')} / {si.get('commercial_area')}")
        brief.append("### ì»¨ì„¤íŒ… ìš”ì•½")
        # í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ë ¤ì„œ í‘œì‹œ
        core_stmt = (P["pd_out"] or {}).get("problem_statement") or "ë¬¸ì œ ì •ì˜ ê²°ê³¼ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤."
        brief.append(f"- **í•µì‹¬ ë¬¸ì œ:** {core_stmt}")
        kpis_from_llm = (P["pd_out"] or {}).get("kpis") or []
        if kpis_from_llm:
            brief.append("- **ì£¼ìš” KPI:** " + ", ".join(kpis_from_llm[:5]))
        brief.append("- ì•„ë˜ **ìµœì¢… ë³´ê³ ì„œ** ì„¹ì…˜ì—ì„œ ìƒì„¸ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.")
        brief_reply = "\n\n".join(brief)

        st.session_state.messages.append(AIMessage(content=brief_reply))
        with st.chat_message("assistant"):
            st.markdown(brief_reply)

        # â‘¡ KPI ì°¨íŠ¸ëŠ” ì±„íŒ… ë©”ì‹œì§€ ë°–(ì•„ë˜) â€” ê°€ë¡œí­ ë¬¸ì œ ë°©ì§€
        if P["chart_b64"]:
            with st.container():
                st.image(base64.b64decode(P["chart_b64"]), use_container_width=True)
                st.caption("ìë™ ìƒì„± KPI ì¶”ì„¸ ì°¨íŠ¸")
                if P["used_kpis"]:
                    st.caption("ì‚¬ìš©ëœ KPI: " + ", ".join(map(str, P["used_kpis"])))

        # â‘¢ ìµœì¢… ë³´ê³ ì„œëŠ” ì±„íŒ… ì•„ë˜ì˜ í° ì„¹ì…˜ì— ì „ì²´ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ
        st.markdown("----")
        st.subheader("ğŸ“˜ ìµœì¢… ë³´ê³ ì„œ (ìƒì„¸)")
        st.markdown(P["report_md"], unsafe_allow_html=True)

        # ì¢…ë£Œ
        P["running"] = False
        _set_step("idle")

# --- ì…ë ¥ì°½ ---
user_query = st.chat_input("ê°€ë§¹ì  ì´ë¦„ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”.")
if user_query:
    st.session_state.user_query = user_query
    st.session_state.messages.append(HumanMessage(content=user_query))
    with st.chat_message("user"):
        st.write(user_query)

    # 1) íŒŒì´í”„ë¼ì¸ì´ ëŒê³  ìˆì§€ ì•Šê³ ,
    # 2) ì´ë¯¸ ë³´ê³ ì„œê°€ ì¡´ì¬í•œë‹¤ë©´ â†’ 'íŒ”ë¡œì—… ì§ˆì˜'ë¡œ ì²˜ë¦¬
    if (not st.session_state.pipeline["running"]) and st.session_state.pipeline.get("report_md"):
        handle_followup(user_query)
    else:
        # ê·¸ ì™¸ì—ëŠ” ìƒˆ ë¶„ì„ìœ¼ë¡œ íŒë‹¨ (ê°€ë§¹ì  ê²€ìƒ‰ í¬í•¨)
        if not st.session_state.pipeline["running"]:
            run_full_pipeline_resumable(user_query, merchant_id=None)

# í›„ë³´ ì„ íƒ ì‹œì—ë„ ì´ì–´ì„œ ì‹¤í–‰
if st.session_state.awaiting_candidate and st.session_state.candidates:
    st.info("ì´ë¦„ì´ ìœ ì‚¬í•œ ê°€ë§¹ì ì´ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    cands = st.session_state.candidates
    idx = st.radio("ê°€ë§¹ì ì„ ì„ íƒí•˜ì„¸ìš”",
                   options=list(range(len(cands))),
                   format_func=lambda i: _fmt_cand_with_id(cands[i]),
                   key="cand_radio")
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ì„ íƒ í™•ì •", key="cand_confirm"):
            sel = cands[idx]
            picked_id = str(sel.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"))
            st.session_state.awaiting_candidate = False
            # ì¬ê°œ: runningì´ ì¼œì ¸ ìˆìœ¼ë©´ merchantë§Œ ì£¼ì…í•˜ê³  ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
            st.session_state.pipeline["merchant_id"] = picked_id
            _set_step("timeseries")
            run_full_pipeline_resumable()
    with col2:
        if st.button("ì„ íƒ ì·¨ì†Œ", key="cand_cancel"):
            st.session_state.awaiting_candidate = False
            st.session_state.candidates = []
            st.session_state.exploration = None

        # ğŸ”’ ì´ í™”ë©´ì´ ë–  ìˆëŠ” ë™ì•ˆ ë’¤ ì½”ë“œë¥¼ ê·¸ë¦¬ì§€ ì•Šë„ë¡ ê°•ì œ ì¢…ë£Œ
    st.stop()
    
# ì‚¬ìš©ìê°€ 'ì „ëµ ì œì•ˆ ì¢…ë£Œ'ë¥¼ ëˆŒë €ë‹¤ë©´ ì¦‰ì‹œ ì¢…ë£Œ
if st.session_state.pipeline["running"] and st.session_state.pipeline.get("cancel_requested"):
    st.session_state.pipeline.update({"running": False, "step": "idle"})
    st.info("â¹ï¸ ì „ëµ ì œì•ˆì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")

# í™”ë©´ì´ ì¬ì‹¤í–‰ë˜ì–´ë„, running=Trueë©´ ê³„ì† ì§„í–‰
if st.session_state.pipeline["running"] and not st.session_state.awaiting_candidate:
    run_full_pipeline_resumable()