import os
import sys
import json
import time
import shutil
import streamlit as st
import google.generativeai as genai
from google.generativeai import protos
import pandas as pd
import numpy as np
from typing import Any, Dict, List, Optional
from pathlib import Path
from PIL import Image
import plotly.express as px
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import logging
from dotenv import load_dotenv

# =========================
# 0) ê³µí†µ ì„¤ì •
# =========================
ASSETS = Path("assets")
st.set_page_config(
    page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬",
    layout="wide",
    page_icon="ğŸ“ˆ",
)

# ê·¸ë˜í”„ ëŒ€ìƒ ì»¬ëŸ¼ ì •ì˜
PLOTTING_NUMERIC_COLS = [
    'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨', 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨',
    'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘', 'ê±°ì£¼ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'
]

@st.cache_data
def load_image(name: str):
    return Image.open(ASSETS / name)

# í—¤ë”
st.title("ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ ğŸ”‘")
st.image(load_image("KMWL.png"), width= 500, caption="ê³ ë¯¼í•˜ì§€ ë§ê³ , AIë¹„ë°€ìƒë‹´ì‚¬ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”!")
st.write("")

# ë¡œê¹… ë° UI í—¬í¼
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
console_logger = logging.getLogger(__name__)

def user_warn(msg: str): st.warning(msg, icon="âš ï¸")
def user_err(msg: str, details: dict = None):
    st.error(msg, icon="ğŸ›‘")
    if details:
        with st.expander("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´"): st.json(details)

# 1. st.secretsì—ì„œ get (1ìˆœìœ„ - Streamlit ë°°í¬ ì‹œ í‘œì¤€ ë°©ì‹)
GOOGLE_API_KEY = st.secrets.get("GOOGLE_API_KEY")

# 2. st.secretsì— ê°’ì´ ì—†ë‹¤ë©´ .env í™œìš©
if not GOOGLE_API_KEY:
    st.warning("âš ï¸ GOOGLE_API_KEYê°€ st.secretsì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .envë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    load_dotenv() # .env íŒŒì¼ì˜ ë³€ìˆ˜ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œë“œ
    GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

# 3. .envì—ë„ ê°’ì´ ì—†ë‹¤ë©´ í•˜ë“œì½”ë”© 
if not GOOGLE_API_KEY:
    st.warning("âš ï¸ GOOGLE_API_KEYê°€ st.secrets ë˜ëŠ” .envì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    GOOGLE_API_KEY = "AIzaSyB8R3nurDOohfAvKXSgBUVRkoliXtfnTKo"

# --- ìµœì¢… í™•ì¸ ë° API ì„¤ì • ---
if GOOGLE_API_KEY:
    # st.success("API í‚¤ ë¡œë“œ ì„±ê³µ.") # (ë””ë²„ê¹… ìš©ë„ë¡œ ì‚¬ìš©)
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    # 1, 2, 3 ìˆœì„œ ëª¨ë‘ ì‹¤íŒ¨í•œ ê²½ìš°
    st.error("ğŸ›‘ ì¹˜ëª…ì  ì˜¤ë¥˜: GOOGLE_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•± ì‹¤í–‰ì„ ì¤‘ì§€í•©ë‹ˆë‹¤.")
    st.stop() # ì•± ì‹¤í–‰ ì¤‘ì§€

# =========================
# 1) Streamlitìš© ë¡œê±°
# =========================
class StreamlitLogHandler:
    def __init__(self):
        if "log_entries" not in st.session_state: st.session_state.log_entries = []
    
    def log(self, message: str, details: dict = None, expander_label: str = "ìƒì„¸ ê²°ê³¼ ë³´ê¸°", update: bool = False):
        entry = {"message": f"- {message}", "details": details, "expander_label": expander_label}
        if update and st.session_state.log_entries: st.session_state.log_entries[-1] = entry
        else: st.session_state.log_entries.append(entry)
        time.sleep(0.5)
    
    def render(self, container):
        container.empty()
        for entry in st.session_state.log_entries:
            container.markdown(entry["message"])
            if entry["details"]:
                with container.expander(entry["expander_label"], expanded=False): st.json(entry["details"])

# ---------------------------
# ì—ì´ì „íŠ¸(ì¶”ë¡ /ë¦¬í¬íŒ…) ë¡œì§
# ---------------------------
class InteractiveParallelAgent:
    """
    ì™¸ë¶€ í™˜ê²½ ë°ì´í„°ë¥¼ í†µí•©í•˜ê³  í† í° íš¨ìœ¨ì ì¸ ë°ì´í„° ì••ì¶• ìš”ì•½ì„ ì ìš©í•œ
    ëŒ€í™”í˜• (ê°€ë§¹ì  ì„ íƒ ì‹œ) ë¹„ë™ê¸° ì²˜ë¦¬ ì—ì´ì „íŠ¸ (ë¬¸ì œ ì •ì˜ ë‹¨ê³„)
    """

    # --- âœ¨ 1ë‹¨ê³„: ë³€í™˜ ìŠ¤í‚¤ë§ˆ ë° í”„ë¡¬í”„íŠ¸ ---
    _TRANSFORM_SCHEMA = protos.Schema(
        type=protos.Type.OBJECT,
        properties={
            'target': protos.Schema(type=protos.Type.STRING, description="ì „ëµ ìš”ì²­ ëŒ€ìƒ (ê°€ë§¹ì  ì´ë¦„)"),
            'challenge': protos.Schema(type= protos.Type.STRING, description="ì‚¬ìš©ìê°€ ì§ë©´í•œ ì–´ë ¤ì›€ì´ë‚˜ ë¬¸ì œ"),
            'objective': protos.Schema(type=protos.Type.STRING, description="ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ëª©ì "),
            'solution_direction': protos.Schema(type=protos.Type.STRING, description="ì œì•ˆë  ìˆ˜ ìˆëŠ” í•´ê²°ì±…ì˜ ë°©í–¥"),
        },
        required=['target', 'challenge', 'objective', 'solution_direction']
    )
    _TRANSFORM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë„¤ ê°€ì§€ í•­ëª©ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- "target": ì‚¬ìš©ìê°€ ì „ëµì„ ìš”ì²­í•œ ëŒ€ìƒì€ ë¬´ì—‡ì¸ê°€ìš”? (ê°€ë§¹ì  ì´ë¦„. ì˜ˆ: 'ì„±ìš°**', 'í–‰*') ('ë§¤ì¥', 'ê°€ë§¹ì 'ê³¼ ê°™ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”)
- "challenge": ì‚¬ìš©ìê°€ ì§ë©´í•œ ì–´ë ¤ì›€ì´ë‚˜ í•´ê²°í•´ì•¼ í•  ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
- "objective": ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
- "solution_direction": ì œì•ˆë  ìˆ˜ ìˆëŠ” í•´ê²°ì±…ì˜ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
"""

    # --- âœ¨ 3ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ìŠ¤í‚¤ë§ˆ ë° í”„ë¡¬í”„íŠ¸ (ì™¸ë¶€ ë°ì´í„° í¬í•¨) ---
    _DEFINE_PROBLEM_SCHEMA = protos.Schema(
        type=protos.Type.OBJECT,
        properties={
            'problem_statement': protos.Schema(type=protos.Type.STRING, description="ë°ì´í„°(ë‚´ë¶€+ì™¸ë¶€) ê¸°ë°˜ìœ¼ë¡œ ì •ì˜ëœ êµ¬ì²´ì ì¸ ì´ìŠˆ"),
            'kpis': protos.Schema(
                type=protos.Type.ARRAY,
                items=protos.Schema(type=protos.Type.STRING),
                description="ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¸¡ì • ê°€ëŠ¥í•œ í•µì‹¬ ì„±ê³¼ ì§€í‘œ ëª©ë¡"
            ),
        },
        required=['problem_statement', 'kpis']
    )
    _DEFINE_PROBLEM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê°€ë§¹ì ì˜ ë‚´ë¶€ ì„±ê³¼ ë°ì´í„°(ì••ì¶•ëœ í†µê³„ ìš”ì•½)ì™€ ì™¸ë¶€ í™˜ê²½ ë°ì´í„°, ê·¸ë¦¬ê³  ìµœì´ˆ ì‚¬ìš©ì ìš”ì²­ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬, ë¬¸ì œ ìƒí™©ê³¼ í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.

**ë°ì´í„° êµ¬ì¡° ì„¤ëª…:**
1.  **ë‚´ë¶€ ì„±ê³¼ ë°ì´í„° (ì••ì¶• ìš”ì•½):** ìµœê·¼ ì‹œê³„ì—´ í†µê³„ ìš”ì•½
    * ê¸°ë³¸ ì •ë³´ (meta): ê°€ë§¹ì ID, ì—…ì¢…, ì—…ì¢…_ë¶„ë¥˜, ìƒê¶Œ, ë¶„ì„ê¸°ê°„, cluster ì •ë³´ ë“±
    * ì „ì²´ í‰ê·  ì§€í‘œ (avg_metrics): ì¥ê¸° í‰ê· ê°’
    * íŠ¸ë Œë“œ (trend): ì´ˆê¸° ëŒ€ë¹„ ìµœê·¼ ì„±ê³¼ ë³€í™”
    * ìµœê·¼ 3ê°œì›” ìƒì„¸ (recent_3months): ì›”ë³„ ìƒì„¸ ë°ì´í„°
    * ì£¼ìš” ì´ë²¤íŠ¸ (significant_events): ê¸‰ê²©í•œ ë³€í™” ì‹œì 
    * êµ¬ê°„ ë¹ˆë„ (range_frequency): ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì„±ê³¼ êµ¬ê°„
2.  **ì™¸ë¶€ í™˜ê²½ ë°ì´í„°:** í•´ë‹¹ ê°€ë§¹ì ì´ ì†í•œ í–‰ì •ë™ ë° ì—…ì¢… ë¶„ë¥˜ì˜ ìƒê¶Œ ë¶„ì„ ì •ë³´ (ì˜ì—…í™˜ê²½ì§€ìˆ˜, ì ì¬ê³ ê°, ê²½ìŸê°•ë„ ë“±) - ì œê³µëœ ê²½ìš° ì°¸ê³ 

**ë°ì´í„° í•´ì„ ê°€ì´ë“œ:**
- ë‚´ë¶€ ë°ì´í„° í•´ì„ ê°€ì´ë“œ (ê¸°ì¡´ê³¼ ë™ì¼):
    - ë§¤ì¶œê¸ˆì•¡/ê±´ìˆ˜/ê³ ê°ìˆ˜ êµ¬ê°„: 1(ìƒìœ„)~6(í•˜ìœ„)
    - ì·¨ì†Œìœ¨ êµ¬ê°„: 1(ë‚®ìŒ, ì¢‹ìŒ)~5(ë†’ìŒ, ë‚˜ì¨)
    - ë™ì¼ ì—…ì¢…/ìƒê¶Œ ë¹„êµ ì§€í‘œ: 100% ê¸°ì¤€, ìˆœìœ„ëŠ” 0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„
    - cluster: 0(ìƒìœ„ê¶Œ ì•ˆì •), 1(í•˜ìœ„ê¶Œ ì•ˆì •), 2(ë³€ë™ì„± ë†’ìŒ), 3(ì‹ ê·œ)
    - cluster_6: ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ (0: ì—¬ì„±40ëŒ€+ê±°ì£¼, 1: ë‚¨ì„±30-40ëŒ€+ì‹ ê·œ, ...)
- ì™¸ë¶€ ë°ì´í„° í•´ì„:
    - ì˜ì—…í™˜ê²½ì§€ìˆ˜(BSENV_NIDX): ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (1:ìµœìš°ìˆ˜ ~ 9:ìœ„í—˜)
    - ì ì¬ê³ ê°ì§€ìˆ˜(PRSPT_NIDX): ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (1:ìµœìš°ìˆ˜ ~ 9:ìœ„í—˜)
    - ê²½ìŸê°•ë„ì§€ìˆ˜(CPITS_NIDX): ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (1:ìµœìš°ìˆ˜ ~ 9:ìœ„í—˜, 99:ì •ë³´ì—†ìŒ)
    - ê° ì§€ìˆ˜ì˜ ì§„ë‹¨ ì½”ë©˜íŠ¸(DGNS_CTT) ì°¸ê³ 

**íŠ¹íˆ ì£¼ëª©í•  ì :**
- ë‚´ë¶€ ë°ì´í„°: íŠ¸ë Œë“œ ë³€í™”ìœ¨, ìµœê·¼ 3ê°œì›” ì¶”ì„¸, ì£¼ìš” ì´ë²¤íŠ¸(ê¸‰ë“±/ê¸‰ë½), combination ì§€í‘œ(clusterì™€ cluster_6 ì •ë³´ë¥¼ ê²°í•©í•œ ê³ ê° ë° ê°€ë§¹ì  ì„¸ê·¸ë¨¼íŠ¸ íŠ¹ì„±)
- ì™¸ë¶€ ë°ì´í„°: ê°€ë§¹ì ì˜ ì˜ì—… í™˜ê²½ ìˆ˜ì¤€ (ì–‘í˜¸, ë³´í†µ, ê´€ì°°, ì£¼ì˜ ë“±), ì ì¬ê³ ê° íŠ¹ì„±, ê²½ìŸ ê°•ë„ ì§„ë‹¨ ë‚´ìš©

[ì¤‘ìš”] KPIs ì„ ì • ì§€ì¹¨
'kpis' ëª©ë¡ì„ ìƒì„±í•  ë•Œ, ë°˜ë“œì‹œ ë‹¤ìŒì˜ **[ìœ íš¨í•œ KPI ì»¬ëŸ¼ ëª©ë¡]**ì— ìˆëŠ” ì´ë¦„ê³¼
**ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”** ë‹¨ì–´ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
ëª©ë¡ì— ì—†ëŠ” ë‹¨ì–´(ì˜ˆ: 'ê°ë‹¨ê°€', 'ì„±ì¥ë¥ ')ë¥¼ KPI ì´ë¦„ìœ¼ë¡œ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.

[ìœ íš¨í•œ KPI ì»¬ëŸ¼ ëª©ë¡]
- 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨'
- 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨'
- 'ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨'
- 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨'
- 'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘'
- 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘'
- 'ê±°ì£¼ ì´ìš© ê³ ê° ë¹„ìœ¨'
- 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨'
- 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'

ê²°ê³¼ë¡œ í˜„ì¬ ì§ë©´í•œ êµ¬ì²´ì ì¸ ì´ìŠˆ(problem_statement)ì™€ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¸¡ì • ê°€ëŠ¥í•œ í•µì‹¬ ì§€í‘œ(kpis)ë¥¼ ë„ì¶œí•˜ì„¸ìš”. **ì™¸ë¶€ í™˜ê²½ ìš”ì¸ì„ ë¬¸ì œ ì •ì˜ì— ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.**
"""

    # --- âœ¨ 4ë‹¨ê³„: ë¦¬í¬íŠ¸ ìƒì„± ìŠ¤í‚¤ë§ˆ ë° í”„ë¡¬í”„íŠ¸ ---
    _GENERATE_STRATEGY_REPORT_SCHEMA = protos.Schema(
        type=protos.Type.OBJECT,
        properties={
            'report': protos.Schema(type=protos.Type.STRING, description="ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ì „ì²´ ìµœì¢… ë³´ê³ ì„œ ë‚´ìš©"),
        },
        required=['report']
    )
    _GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT = """
    ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì´ì ì „ë¬¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 'ë¬¸ì œ ì •ì˜'(ë‚´ë¶€ ë°ì´í„°ì™€ ì™¸ë¶€ í™˜ê²½ ìš”ì¸ í¬í•¨)ì™€ ì••ì¶•ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, ìµœì¢… ì‚¬ìš©ìë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì‚¬ìš©ìì˜ ì£¼ìš” ìš”ì²­ ë°©í–¥ ('solution_direction'):** {solution_direction}

ë³´ê³ ì„œëŠ” ìœ„ **ìš”ì²­ ë°©í–¥ì— ë¶€í•©í•˜ëŠ”** êµ¬ì²´ì ì¸ ì „ëµê³¼ ì‹¤í–‰ ê³„íšì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ë°ì´í„°ì™€ ì§ì ‘ ì—°ê²°ëœ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”. ì™¸ë¶€ í™˜ê²½ ë°ì´í„°ì—ì„œ ë„ì¶œëœ ì¸ì‚¬ì´íŠ¸ë„ ì „ëµì— ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

**ì£¼ì˜ì‚¬í•­:**
- ê°€ë§¹ì ì˜ ì´ë¦„ì„ ì„ì˜ë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
- 16384 í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬, ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì „ëµì  ì œì•ˆ ë° ì‹¤í–‰ ê³„íšì—ì„œ ëª¨ë“  ì œì•ˆê³¼ ë‹µë³€ì—ëŠ” ê·¼ê±°ê°€ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¨, KPIì™€ ì¼ë°˜ì ì¸ ì§€ì‹ì€ ê·¼ê±°ë¡œ í‘œì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì œê³µëœ ë°ì´í„°ì— ê·¼ê±°í•˜ì§€ ì•Šì€ ì¶”ìƒì ì´ê±°ë‚˜ ì¼ë°˜ì ì¸ ì¡°ì–¸ì€ í”¼í•˜ì„¸ìš”.
- ë‹¨ìˆœíˆ ë°ì´í„°ë¥¼ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë°ì´í„°ë¥¼ í•´ì„í•˜ì—¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
- ë³´ê³ ì„œ ì„œë‘ì— ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ì´ë‚˜ ìê¸°ì†Œê°œëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ì™¸ë¶€ í™˜ê²½ì— ëŒ€í•œ ë°ì´í„°ì— ëŒ€í•œ ì¶œì²˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•˜ì„¸ìš” : (ì¶œì²˜ : ì‹ ìš©ë³´ì¦ê¸°ê¸ˆ BASA ì†Œìƒê³µì¸ ìƒê¶Œë¶„ì„ ë°ì´í„°)
- ì´ˆê¸° 6ê°œì›” í‰ê· ê³¼ ìµœê·¼ 6ê°œì›” í‰ê·  ì„±ê³¼ë¥¼ ë¹„êµ ë¶„ì„í•  ë•ŒëŠ” **ë°˜ë“œì‹œ Markdown í…Œì´ë¸” í˜•ì‹**ì„ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•˜ê²Œ ì œì‹œí•©ë‹ˆë‹¤. í…Œì´ë¸”ì—ëŠ” 'ì§€í‘œ', 'ì´ˆê¸° 6ê°œì›” í‰ê· ', 'ìµœê·¼ 6ê°œì›” í‰ê· ', 'ë³€í™”ìœ¨', 'ì¸ì‚¬ì´íŠ¸' ì»¬ëŸ¼ì„ í¬í•¨í•˜ì„¸ìš”. ìµœê·¼ 3ê°œì›” ë™í–¥ê³¼ ì™¸ë¶€ í™˜ê²½ ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

ìµœì¢… ê²°ê³¼ëŠ” ì „ì²´ ë³´ê³ ì„œ ë‚´ìš©ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
**ë‹¹ì‹ ì˜ ìµœì¢… ë³´ê³ ì„œëŠ” ë°˜ë“œì‹œ 'ì‚¬ìš©ìì˜ ì£¼ìš” ìš”ì²­ ë°©í–¥': '{solution_direction}'ì„(ë¥¼) ì¶©ì¡±í•˜ëŠ” ì „ëµì„ ìƒì„¸í•˜ê²Œ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤.**

"""

    def __init__(self, sync_model: Any, csv_path: str, json_data: List[Dict[str, Any]]):
        self.model = sync_model
        self.context = {}
        self.json_data = json_data
        self._last_external_data = None

        try:
            self.df = pd.read_csv(csv_path)
            self.df.columns = self.df.columns.str.strip()

            if "ê¸°ì¤€ë…„ì›”" in self.df.columns:
                self.df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(self.df["ê¸°ì¤€ë…„ì›”"].astype(str).str.zfill(6), format="%Y%m", errors='coerce')
            console_logger.info(f"âœ… CSV ë°ì´í„° ë¡œë“œ ì„±ê³µ: {csv_path} ({len(self.df)} rows)")
            required_csv_cols = ['ê°€ë§¹ì ëª…', 'ì—…ì¢…', 'ì—…ì¢…_ë¶„ë¥˜', 'ìƒê¶Œ', 'í–‰ì •ë™', 'ê¸°ì¤€ë…„ì›”']
            
            if not all(col in self.df.columns for col in required_csv_cols):
                console_logger.warning(f"âš ï¸ CSV íŒŒì¼ì— ì¼ë¶€ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        except FileNotFoundError:
            console_logger.error(f"âŒ ì˜¤ë¥˜: CSV íŒŒì¼ '{csv_path}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.df = pd.DataFrame()
        
        except Exception as e:
            console_logger.error(f"âŒ CSV ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.df = pd.DataFrame()

    def _generate_content_sync(self, prompt: str, response_schema: Optional[protos.Schema] = None, generation_config: Optional[Dict[str, Any]] = None, retries: int = 3, delay: int = 5) -> Dict[str, Any]:
        last_exception = None
        for i in range(retries):
            try:
                final_config_dict = generation_config.copy() if generation_config else {}
                if response_schema:
                    final_config_dict["response_schema"] = response_schema
                    final_config_dict["response_mime_type"] = "application/json"
                
                # protos.GenerationConfig ê°ì²´ë¡œ ë³€í™˜
                final_gen_config = genai.types.GenerationConfig(**final_config_dict)

                time.sleep(1)
                response = self.model.generate_content(prompt, generation_config=final_gen_config)

                if response.candidates and response.candidates[0].finish_reason != 1:
                    reason_map = {0: "UNKNOWN", 1: "STOP", 2: "MAX_TOKENS", 3: "SAFETY", 4: "RECITATION", 5: "OTHER"}
                    reason_str = reason_map.get(response.candidates[0].finish_reason, "ê¸°íƒ€")
                    raise ValueError(f"API í˜¸ì¶œ ë¹„ì •ìƒ ì¢…ë£Œ (Reason: {reason_str})")
                
                if not response.text:
                    safety_ratings = response.candidates[0].safety_ratings if response.candidates else "N/A"
                    raise ValueError(f"API ì‘ë‹µ ë‚´ìš© ë¹„ì–´ ìˆìŒ (Safety: {safety_ratings})")
                return json.loads(response.text)
            
            except (json.JSONDecodeError, ValueError, Exception) as e:
                last_exception = e
                if i == retries - 1: break
                wait_time = delay * (2 ** i)
                console_logger.warning(f"âš ï¸ LLM ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨ ({e}). {wait_time}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {i + 1}/{retries})")
                time.sleep(wait_time)
        return {"error": f"LLM ë™ê¸° í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨ ({retries}ë²ˆ ì¬ì‹œë„): {last_exception}"}

    def _transform(self, initial_input: str):
        console_logger.info("1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ë³€í™˜ ì‹œì‘...")
        target_name_guess = initial_input.split()[0] if initial_input else "ì•Œ ìˆ˜ ì—†ëŠ” ê°€ë§¹ì "
        user_prompt = f"ğŸ”¹ ì‚¬ìš©ì ì…ë ¥:\nëŒ€ìƒ (ê°€ë§¹ì  ì´ë¦„): '{target_name_guess}'\nìš”ì²­ ë‚´ìš©: {initial_input}"
        full_prompt = f"{self._TRANSFORM_SYSTEM_PROMPT}\n\n{user_prompt}"
        self.context['transformation'] = self._generate_content_sync(full_prompt, response_schema=self._TRANSFORM_SCHEMA, generation_config={"max_output_tokens": 1024})
        console_logger.info("1ë‹¨ê³„: ë³€í™˜ ì™„ë£Œ.")
    
    def _find_and_clarify_store(self):
        console_logger.info("2-1ë‹¨ê³„: ê°€ë§¹ì  ê²€ìƒ‰ ì‹œì‘...")
        transformation_result = self.context.get('transformation', {})
        target_description = transformation_result.get('target', '')
        if "error" in transformation_result:
            self.context['error'] = f"1ë‹¨ê³„ ë³€í™˜ ê²°ê³¼ ì˜¤ë¥˜: {transformation_result['error']}"; return
        if not target_description:
            self.context['error'] = "1ë‹¨ê³„ ë³€í™˜ ê²°ê³¼ì—ì„œ 'target' ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."; return
        search_term = target_description.replace('*', '').strip()
        required_cols = ['ê°€ë§¹ì ëª…', 'ì—…ì¢…', 'ì—…ì¢…_ë¶„ë¥˜', 'ìƒê¶Œ', 'í–‰ì •ë™'] # âœ… 'ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸' ì¶”ê°€ í•„ìš”
        if self.df.empty or not all(col in self.df.columns for col in required_cols + ['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸']):
             self.context['error'] = "CSV ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ ìˆê±°ë‚˜ í•„ìˆ˜ ì»¬ëŸ¼ ë¶€ì¡±"; return
        try:
            candidate_stores = self.df[self.df['ê°€ë§¹ì ëª…'].str.contains(search_term, na=False)]
            # âœ… ê°€ë§¹ì  IDë„ í¬í•¨í•˜ì—¬ ì¤‘ë³µ ì œê±°
            unique_candidates = candidate_stores[required_cols + ['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸']].drop_duplicates().to_dict('records')
        except Exception as e:
             self.context['error'] = f"ê°€ë§¹ì  ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"; return

        if not unique_candidates:
            self.context['error'] = f"'{search_term}'(ì„)ë¥¼ í¬í•¨í•˜ëŠ” ê°€ë§¹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        elif len(unique_candidates) == 1:
            self.context['target_store_info'] = unique_candidates[0]
            console_logger.info(f"âœ… ê°€ë§¹ì  ìœ ì¼ ì‹ë³„: {unique_candidates[0]['ê°€ë§¹ì ëª…']}")
        else:
            self.context['clarification_needed'] = unique_candidates
            console_logger.warning(f"âš ï¸ '{search_term}'ì— ëŒ€í•´ {len(unique_candidates)}ê°œ í›„ë³´ ë°œê²¬")
        console_logger.info("2-1ë‹¨ê³„: ê°€ë§¹ì  ê²€ìƒ‰ ì™„ë£Œ.")

    def _compress_store_data(self):
        console_logger.info("2-2ë‹¨ê³„: ë°ì´í„° ì••ì¶• ìš”ì•½ ì‹œì‘...")
        store_info = self.context.get('target_store_info')
        if not store_info: self.context['error'] = "ê°€ë§¹ì  ì •ë³´ ë¯¸í™•ì •"; return
        try:
            store_data = self.df[
                (self.df['ê°€ë§¹ì ëª…'] == store_info['ê°€ë§¹ì ëª…']) &
                (self.df['ì—…ì¢…'] == store_info['ì—…ì¢…']) &
                (self.df['ì—…ì¢…_ë¶„ë¥˜'] == store_info['ì—…ì¢…_ë¶„ë¥˜']) &
                (self.df['ìƒê¶Œ'] == store_info['ìƒê¶Œ']) &
                (self.df['í–‰ì •ë™'] == store_info['í–‰ì •ë™']) # í–‰ì •ë™ ì¡°ê±´ ì¶”ê°€
            ].sort_values(by='ê¸°ì¤€ë…„ì›”').reset_index(drop=True).copy()
            self.context['target_store_timeseries'] = store_data
        except KeyError as e: self.context['error'] = f"í•„ìš” ì»¬ëŸ¼({e}) ì—†ìŒ"; return
        except Exception as e: self.context['error'] = f"ë°ì´í„° í•„í„°ë§ ì˜¤ë¥˜: {e}"; return
        if store_data.empty: self.context['error'] = "í•´ë‹¹ ê°€ë§¹ì  ë°ì´í„° ì—†ìŒ"; return

        # --- ì••ì¶• ë¡œì§ ---
        meta, avg_metrics, trend = {}, {}, {}
        recent_3_detailed, significant_events, range_frequency = [], [], {}
        try:
            first_row = store_data.iloc[0]
            meta = {
                'ê°€ë§¹ì ID': str(first_row.get('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸')), 'ê°€ë§¹ì ëª…': first_row.get('ê°€ë§¹ì ëª…'),
                'ì—…ì¢…': first_row.get('ì—…ì¢…'), 'ì—…ì¢…_ë¶„ë¥˜': first_row.get('ì—…ì¢…_ë¶„ë¥˜'),
                'ìƒê¶Œ': first_row.get('ìƒê¶Œ'), 'í–‰ì •ë™': first_row.get('í–‰ì •ë™'),
                'ë¶„ì„ê¸°ê°„': f"{store_data['ê¸°ì¤€ë…„ì›”'].min().strftime('%Y-%m')} ~ {store_data['ê¸°ì¤€ë…„ì›”'].max().strftime('%Y-%m')}",
                'ë°ì´í„°í¬ì¸íŠ¸': len(store_data),
                'cluster': int(first_row['cluster']) if 'cluster' in first_row and pd.notna(first_row['cluster']) else None,
                'cluster_6': int(first_row['cluster_6']) if 'cluster_6' in first_row and pd.notna(first_row['cluster_6']) else None
            }
            # target_store_info ì—…ë°ì´íŠ¸ (ê°€ë§¹ì ID í¬í•¨)
            self.context['target_store_info'].update({'ê°€ë§¹ì ID': meta['ê°€ë§¹ì ID'], 'í–‰ì •ë™': meta['í–‰ì •ë™']})

            potential_numeric_cols = PLOTTING_NUMERIC_COLS + ['ë™ì¼ ì—…ì¢… ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘', 'ë™ì¼ ìƒê¶Œ ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘'] # í•´ì§€ ë¹„ì¤‘ í¬í•¨
            numeric_cols = [col for col in potential_numeric_cols if col in store_data.columns]
            store_data_clean = store_data[numeric_cols].apply(pd.to_numeric, errors='coerce').replace([-999999.9, -999999], np.nan)

            avg_metrics = {col: round(store_data_clean[col].mean() or 0, 2) for col in numeric_cols if store_data_clean[col].notna().any()}
            
            if len(store_data_clean) >= 12:
                first_6, last_6 = store_data_clean.head(6), store_data_clean.tail(6)
                for col in numeric_cols:
                    first_val, last_val = first_6[col].mean(), last_6[col].mean()
                    if pd.notna(first_val) and pd.notna(last_val) and abs(first_val) > 1e-6:
                        change_pct = ((last_val - first_val) / abs(first_val)) * 100
                        trend[col] = {'ì´ˆê¸°_í‰ê· ': round(first_val, 2), 'ìµœê·¼_í‰ê· ': round(last_val, 2), 'ë³€í™”ìœ¨': f"{change_pct:+.1f}%"}
            
            # ê¸°ì¤€ë…„ì›” ì»¬ëŸ¼ì„ intë¡œ ë³€í™˜ ì‹œë„
            if 'ê¸°ì¤€ë…„ì›”' in store_data.columns:
                 store_data['ê¸°ì¤€ë…„ì›”_int'] = store_data['ê¸°ì¤€ë…„ì›”'].dt.strftime('%Y%m').astype(int)

            recent_3 = store_data.tail(3)
            for _, row in recent_3.iterrows():
                 month_key = int(row['ê¸°ì¤€ë…„ì›”_int']) if 'ê¸°ì¤€ë…„ì›”_int' in row and pd.notna(row['ê¸°ì¤€ë…„ì›”_int']) else f"Row_{_}"
                 month_data = {'ê¸°ì¤€ë…„ì›”': month_key}
                 month_data.update({col: round(row[col], 2) for col in numeric_cols if col in row and pd.notna(row[col])})
                 recent_3_detailed.append(month_data)

            event_cols = [col for col in ['ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'] if col in numeric_cols]
            if 'ê¸°ì¤€ë…„ì›”_int' in store_data.columns:
                for col in event_cols:
                    values = store_data[[col, 'ê¸°ì¤€ë…„ì›”_int']].dropna().copy()
                    if len(values) >= 2:
                        values[col] = pd.to_numeric(values[col], errors='coerce').dropna()
                        if len(values) >= 2:
                            values['pct_change'] = values[col].pct_change() * 100
                            events = values[abs(values['pct_change']) > 15].copy()
                            for idx in events.index:
                                if idx > values.index.min():
                                    prev_idx = values.index[values.index.get_loc(idx) - 1]
                                    prev_row, event_row = values.loc[prev_idx], values.loc[idx]
                                    significant_events.append({'ì§€í‘œ': col,'ì‹œì ': int(event_row['ê¸°ì¤€ë…„ì›”_int']),'ë³€í™”': f"{prev_row[col]:.1f} â†’ {event_row[col]:.1f} ({event_row['pct_change']:+.1f}%)"})

            potential_range_cols = ['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„', 'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„', 'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„', 'ê°ë‹¨ê°€ êµ¬ê°„', 'ì·¨ì†Œìœ¨ êµ¬ê°„']
            range_cols = [col for col in potential_range_cols if col in store_data.columns]
            range_frequency = {col: store_data[col].mode()[0] for col in range_cols if not store_data[col].mode().empty}
        except Exception as e:
            self.context['error'] = f"ë°ì´í„° ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}"; return

        self.context['exploration'] = {
            'meta': meta, 'avg_metrics': avg_metrics, 'trend': trend,
            'recent_3months': recent_3_detailed, 'significant_events': significant_events, 'range_frequency': range_frequency
        }
        console_logger.info("2-2ë‹¨ê³„: ë°ì´í„° ì••ì¶• ì™„ë£Œ.")
    
    def _fetch_external_data(self, dong_name: Optional[str], biz_category: Optional[str]) -> Optional[Dict[str, Any]]:
        self._last_external_data = None
        if not dong_name or not biz_category or not self.json_data:
            console_logger.warning(f"âš ï¸ ì™¸ë¶€ ë°ì´í„° ê²€ìƒ‰ ê±´ë„ˆë›°ê¸°: ì •ë³´ ë¶€ì¡± ë˜ëŠ” JSON ë°ì´í„° ì—†ìŒ")
            return None
        try: last_biz_word = biz_category.split('>')[-1].strip()
        except Exception: return None
        if not last_biz_word: return None
        processed_dong_name = dong_name.replace('ì œ', '').replace('.', '').strip() if dong_name else ""
        console_logger.info(f"ğŸ” ì™¸ë¶€ ë°ì´í„° ê²€ìƒ‰ (í–‰ì •ë™: '{dong_name}', ì—…ì¢…í‚¤ì›Œë“œ: '{last_biz_word}')...")
        for item in self.json_data:
            json_dong_raw = item.get('INPUT_ADUN_NM')
            json_dong_processed = json_dong_raw.replace('ì œ', '').replace('.', '').strip() if json_dong_raw else ""
            json_biz_name = item.get('INPUT_BZC_NM')
            if json_dong_processed and json_biz_name and processed_dong_name == json_dong_processed and last_biz_word == json_biz_name:
                console_logger.info(f"âœ… ì™¸ë¶€ ë°ì´í„° ì°¾ìŒ: {json_dong_raw}, {json_biz_name}")
                self._last_external_data = item; return item
        console_logger.warning(f"âš ï¸ ì™¸ë¶€ ë°ì´í„° ì°¾ì„ ìˆ˜ ì—†ìŒ.")
        return None

    def _define_problem(self, exploration_result: Dict[str, Any], transformation_result: Dict[str, Any], external_data: Optional[Dict[str, Any]]):
        console_logger.info("3ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì‹œì‘...")
        external_data_prompt_part = "ì™¸ë¶€ í™˜ê²½ ë°ì´í„°: í•´ë‹¹ ì§€ì—­/ì—…ì¢… ì •ë³´ ì—†ìŒ"
        if external_data:
            relevant_keys = ["ADUN_FULL_NM", "SMMB_BZC_SCLS_CD_NM", "BSENV_NIDX", "BSENV_DGNS_CTT", "PRSPT_NIDX", "PRSPT_DGNS_CTT", "CPITS_NIDX", "CPITS_DGNS_CTT"]
            filtered_data = {k: external_data.get(k) for k in relevant_keys if external_data.get(k) is not None}
            if filtered_data: external_data_prompt_part = f"ì™¸ë¶€ í™˜ê²½ ë°ì´í„°:\n{json.dumps(filtered_data, indent=2, ensure_ascii=False)}"
            else: external_data_prompt_part = "ì™¸ë¶€ í™˜ê²½ ë°ì´í„°: ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨"
        user_prompt = f"ğŸ”¹ ë¬¸ë§¥(Context):\n- ìµœì´ˆ ìš”ì²­: {json.dumps(transformation_result, indent=2, ensure_ascii=False)}\n- ì••ì¶•ëœ ë‚´ë¶€ ì„±ê³¼ ë°ì´í„°: {json.dumps(exploration_result, indent=2, ensure_ascii=False)}\n- {external_data_prompt_part}"
        full_prompt = f"{self._DEFINE_PROBLEM_SYSTEM_PROMPT}\n\n{user_prompt}"
        response = self._generate_content_sync(full_prompt, response_schema=self._DEFINE_PROBLEM_SCHEMA, generation_config={"max_output_tokens": 8000})
        self.context['problem_definition'] = response
        console_logger.info("3ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì™„ë£Œ.")

    def _generate_strategy_and_report(self) -> str:
        console_logger.info("4ë‹¨ê³„: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘...")
        problem_def = self.context.get('problem_definition', {})
        exploration_res = self.context.get('exploration', {})
        transformation_res = self.context.get('transformation')
        if not transformation_res or "error" in transformation_res: return f"# ë³´ê³ ì„œ ì‹¤íŒ¨: 1ë‹¨ê³„ ì˜¤ë¥˜"
        if not exploration_res or "error" in self.context: return f"# ë³´ê³ ì„œ ì‹¤íŒ¨: 2ë‹¨ê³„ ì˜¤ë¥˜"
        if not problem_def or "error" in problem_def: return f"# ë³´ê³ ì„œ ì‹¤íŒ¨: 3ë‹¨ê³„ ì˜¤ë¥˜"

        solution_direction = transformation_res.get('solution_direction', 'êµ¬ì²´ì ì¸ ì „ëµ ì œì•ˆ ë° ì‹¤í–‰ ê³„íš ì œì‹œ')

        external_summary = "ì •ë³´ ì—†ìŒ"
        if self._last_external_data:
             ext = self._last_external_data; parts = []
             if ext.get('BSENV_DGNS_CTT'): parts.append(f"ì˜ì—…í™˜ê²½: {ext['BSENV_DGNS_CTT']} (ì§€ìˆ˜: {ext.get('BSENV_NIDX', 'N/A')})")
             if ext.get('PRSPT_DGNS_CTT'): parts.append(f"ì ì¬ê³ ê°: {ext['PRSPT_DGNS_CTT']} (ì§€ìˆ˜: {ext.get('PRSPT_NIDX', 'N/A')})")
             if ext.get('CPITS_DGNS_CTT'):
                 idx = ext.get('CPITS_NIDX', 'N/A'); idx_str = 'ì •ë³´ì—†ìŒ' if idx == 99 else str(idx)
                 parts.append(f"ê²½ìŸê°•ë„: {ext['CPITS_DGNS_CTT']} (ì§€ìˆ˜: {idx_str})")
             if parts: external_summary = ", ".join(parts)
             

        final_context = {
            "1_ì´ˆê¸°_ìš”ì²­ì‚¬í•­": transformation_res, "2_ë°ì´í„°ê¸°ë°˜_ë¬¸ì œì •ì˜": problem_def,
            "3_ì£¼ìš”_ë°ì´í„°_ì¸ì‚¬ì´íŠ¸": {
                "ê°€ë§¹ì _ì •ë³´": exploration_res.get('meta'), "ì„±ê³¼_íŠ¸ë Œë“œ": exploration_res.get('trend'),
                "ìµœê·¼_3ê°œì›”_ë™í–¥": exploration_res.get('recent_3months'), "ì™¸ë¶€_í™˜ê²½_ìš”ì•½": external_summary
            }
        }
        user_prompt = f"ğŸ”¹ ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•œ ì¢…í•© ì»¨í…ìŠ¤íŠ¸:\n{json.dumps(final_context, indent=2, ensure_ascii=False)}"
        full_prompt = f"{self._GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT}\n\n{user_prompt}"
        final_response = self._generate_content_sync(full_prompt, response_schema=self._GENERATE_STRATEGY_REPORT_SCHEMA, generation_config={"max_output_tokens": 16384, "temperature": 0.3})
        if "error" in final_response: return f"# ë³´ê³ ì„œ ì‹¤íŒ¨: LLM ì˜¤ë¥˜ {final_response['error']}"
        console_logger.info("4ë‹¨ê³„: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ.")
        return final_response.get("report", "# ë³´ê³ ì„œ ì‹¤íŒ¨: ë‚´ìš© ì—†ìŒ")
    

# ---------------------------
# Streamlit UI
# ---------------------------
system_prompt = (
    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , "
    "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•ê³¼ ì±„ë„, ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. "
    "ê²°ê³¼ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ, ë¶„ì„ ê²°ê³¼ì—ëŠ” ê°€ëŠ¥í•œ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
greeting = "ê°€ë§¹ì  ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜, ê³ ë¯¼ì„ ë§ì”€í•´ì£¼ì„¸ìš”. AIë¹„ë°€ìƒë‹´ì‚¬ê°€ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!"

def ensure_state():
    ss = st.session_state
    if "messages" not in ss: ss.messages = [AIMessage(content=greeting)]
    if "pipeline" not in ss:
        ss.pipeline = {"running": False, "step": "idle", "user_query": "", "trans": None,
                    "merchant_id": None, "exploration": None, "pd_out": None, "report_md": None, "timeseries_data": None}
    ss.setdefault("awaiting_candidate", False)
    ss.setdefault("candidates", [])

ensure_state()

def clear_chat_history():
    st.session_state.messages = [AIMessage(content=greeting)]
    keys_to_clear = ["pipeline", "awaiting_candidate", "candidates", "log_entries"]
    for key in keys_to_clear:
        if key in st.session_state: del st.session_state[key]
    ensure_state() # ìƒíƒœ ì¬ì´ˆê¸°í™”

# í›„ì† ì§ˆë¬¸ ì²˜ë¦¬   
def handle_followup(user_question: str) -> str:
    P = st.session_state.pipeline
    context_for_followup = {
        "previous_report": P.get("report_md"),
        "problem_definition": P.get("pd_out"),
        "data_summary": P.get("exploration"),
    }
    prompt = (
        "ë‹¹ì‹ ì€ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "ìˆ«ìëŠ” í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ ëª…í™•íˆ, ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¶€ì¡±í•˜ë‹¤ê³  ë°í˜€ì£¼ì„¸ìš”.\n\n"
        f"### ì»¨í…ìŠ¤íŠ¸\n{json.dumps(context_for_followup, ensure_ascii=False, indent=2)}\n\n"
        f"### ì‚¬ìš©ìì˜ ì§ˆë¬¸\n{user_question}\n\n"
        "### ë‹µë³€"
    )
    try:
        response = genai.GenerativeModel('gemini-2.5-flash').generate_content(prompt)
        return response.text
    except Exception as e: return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# í›„ë³´ í‘œì‹œ í¬ë§·
def _id_of(cand: dict):
    return cand.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸")

def _fmt_cand_with_id(c: dict) -> str:
    id_val = _id_of(c) or "ID?"
    name = c.get('ê°€ë§¹ì ëª…', '?')
    biz_cat = c.get('ì—…ì¢…_ë¶„ë¥˜', c.get('ì—…ì¢…', '?')) 
    area = c.get('ìƒê¶Œ', '?')
    dong = c.get('í–‰ì •ë™', '?')
    return f"[{id_val}] {name} ({biz_cat} / {area} / {dong})"

def _set_step(step: str): st.session_state.pipeline["step"] = step

# =========================
# 5) ì‚¬ì´ë“œë°”
# =========================
with st.sidebar:
    logo = ASSETS / "shc_ci_basic_00.png"
    if logo.exists():
        st.image(load_image("shc_ci_basic_00.png"), width='stretch')
    st.markdown("<p style='text-align: center;'>2025 Big Contest â€¢ AI DATA í™œìš©</p>", unsafe_allow_html=True)
    st.button("Clear Chat History", on_click=clear_chat_history)

    if st.session_state.pipeline.get("running"):
        if st.button("ë¶„ì„ ì¤‘ë‹¨", type="secondary"): st.session_state.pipeline["cancel_requested"] = True
        
# --- íˆìŠ¤í† ë¦¬ ë Œë” ---
for msg in st.session_state.messages:
    role = "user" if isinstance(msg, HumanMessage) else "assistant"
    with st.chat_message(role): st.write(msg.content)

# --- KPI ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜ ---
def create_kpi_charts(kpi_list: list, timeseries_df: Optional[pd.DataFrame]) -> list:
    """
    AIê°€ ì„ ì •í•œ KPI ë¦¬ìŠ¤íŠ¸ì™€ ì‹œê³„ì—´ ë°ì´í„°í”„ë ˆì„ì„ ë°›ì•„ Plotly ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    PLOTTING_NUMERIC_COLSì— í¬í•¨ëœ ë³€ìˆ˜ë§Œ ê·¸ë˜í”„ë¡œ ê·¸ë¦½ë‹ˆë‹¤. (ë§¤ì¹­ ë¡œì§ ê°œì„ )
    """
    charts = []
    if timeseries_df is None or timeseries_df.empty:
        console_logger.warning("ğŸ“ˆ KPI ì°¨íŠ¸ ìƒì„± ê±´ë„ˆë›°ê¸°: ì‹œê³„ì—´ ë°ì´í„° ì—†ìŒ")
        return charts 

    valid_kpi_columns = []
    for col in PLOTTING_NUMERIC_COLS: 
        if col in timeseries_df.columns: 
            for kpi_item in kpi_list: 
                if col in str(kpi_item): 
                    valid_kpi_columns.append(col)
                    break 

    # ë””ë²„ê¹…ìš© ë¡œê·¸ ì¶”ê°€
    console_logger.info(f"ğŸ“ˆ AI KPI ëª©ë¡: {kpi_list}")
    console_logger.info(f"ğŸ“ˆ ê·¸ë˜í”„ ëŒ€ìƒ ì»¬ëŸ¼ í•„í„°ë§ ê²°ê³¼: {valid_kpi_columns}")

    plotted_cols_count = 0
    for kpi_col in sorted(set(valid_kpi_columns)):
        if plotted_cols_count >= 4: break
        try:
            # âœ… [ìˆ˜ì •] ê·¸ë˜í”„ ê·¸ë¦¬ê¸° ì „ì— ì´ìƒì¹˜(-999999 ë“±)ë¥¼ np.nanìœ¼ë¡œ ëŒ€ì²´
            df_cleaned = timeseries_df.copy()
            df_cleaned[kpi_col] = pd.to_numeric(df_cleaned[kpi_col], errors='coerce') # ìˆ«ìë¡œ ë³€í™˜
            df_cleaned[kpi_col] = df_cleaned[kpi_col].replace([-999999.9, -999999], np.nan) # ì´ìƒì¹˜ NaN ì²˜ë¦¬

            # NaN ì•„ë‹Œ ê°’ì´ í•˜ë‚˜ë¼ë„ ìˆì„ ë•Œë§Œ ê·¸ë˜í”„ ìƒì„±
            if df_cleaned[kpi_col].notna().any():
                fig = px.line(
                    df_cleaned, # ì •ì œëœ ë°ì´í„° ì‚¬ìš©
                    x='ê¸°ì¤€ë…„ì›”',
                    y=kpi_col,
                    title=f'<b>{kpi_col} ì¶”ì´</b>',
                    markers=True,
                    labels={'ê¸°ì¤€ë…„ì›”': 'ì›”', kpi_col: 'ê°’'}
                )
                fig.update_layout(
                    title_font_size=16, title_x=0.5,
                    xaxis_title_font_size=12, yaxis_title_font_size=12,
                    legend_title_text=''
                )
                charts.append(fig)
                plotted_cols_count += 1
            else:
                 console_logger.warning(f"ğŸ“ˆ '{kpi_col}' ì»¬ëŸ¼ì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ê·¸ë˜í”„ ìƒì„± ê±´ë„ˆëœ€")

        except Exception as e:
            console_logger.error(f"ğŸ“ˆ Plotly ê·¸ë˜í”„ ìƒì„± ì˜¤ë¥˜ ({kpi_col}): {e}")

    return charts

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ---
# âœ… ë°ì´í„° ë¡œë”© ë° Agent ì´ˆê¸°í™” (ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ)
@st.cache_resource
def load_data_and_init_agent():
    try:
        csv_path = "./data/labeling_with_geo.csv"
        json_path = "./data/api_results.json"  
        
        with open(json_path, 'r', encoding='utf-8') as f:
            json_content = json.load(f)
        
        sync_model = genai.GenerativeModel('gemini-2.5-flash') 
        
        agent = InteractiveParallelAgent(sync_model, csv_path, json_content)
        
        if agent.df is None or agent.df.empty:
            st.error(f"CSV ë°ì´í„° ë¡œë”© ì‹¤íŒ¨. íŒŒì¼ í™•ì¸: {csv_path}"); st.stop()
        console_logger.info("ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return agent
    
    except FileNotFoundError as e:
        st.error(f"í•„ìˆ˜ íŒŒì¼({e.filename}) ì—†ìŒ. ì•± ì¢…ë£Œ."); st.stop()
    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì˜¤ë¥˜: {e}"); st.stop()

agent = load_data_and_init_agent()

def perform_analysis_sync(agent_instance):
    """
    ë¹„ë™ê¸°(async) ë¡œì§ì„ ëª¨ë‘ ì œê±°í•˜ê³  ìˆœì°¨ì (sync)ìœ¼ë¡œ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    store_info = agent_instance.context.get('target_store_info')
    if not store_info:
        raise RuntimeError("Target store information is missing.")

    # 1. ë‚´ë¶€ ë°ì´í„° ì••ì¶• (ë™ê¸°)
    agent_instance._compress_store_data()

    # _compress_store_data ì‹¤í–‰ í›„ ì˜¤ë¥˜ í™•ì¸
    if "error" in agent_instance.context and "exploration" not in agent_instance.context:
        # ë°ì´í„° ì••ì¶• ë‹¨ê³„ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜ë¥¼ ëª…í™•íˆ ì „ë‹¬í•©ë‹ˆë‹¤.
        raise RuntimeError(f"Internal data compression failed: {agent_instance.context['error']}")

    # 2. ì™¸ë¶€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë™ê¸°)
    fetched_external_data = agent_instance._fetch_external_data(store_info.get('í–‰ì •ë™'), store_info.get('ì—…ì¢…_ë¶„ë¥˜'))
    
    # 3. ë°ì´í„° ê¸°ë°˜ ë¬¸ì œ ì •ì˜ (ë™ê¸°)
    agent_instance._define_problem(agent_instance.context['exploration'], agent_instance.context['transformation'], fetched_external_data)

def run_full_pipeline_resumable(user_text: str | None = None):
    P = st.session_state.pipeline
    logger = StreamlitLogHandler()
    
    if not P.get("running"):
        if user_text is None: return
        P.update({"running": True, "step": "transform", "user_query": user_text})
        agent.context = {}

    if P["step"] == "transform":
        if not st.session_state.get("log_entries"): logger.log("ğŸ¤” 1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì¤‘...")
        agent._transform(P["user_query"])
        trans = agent.context.get("transformation", {})
        if "error" in trans: user_err("ìš”ì²­ ë¶„ì„ ì‹¤íŒ¨", details=trans); P["running"] = False; _set_step("idle"); return
        P["trans"] = trans
        logger.log("âœ… 1ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ì™„ë£Œ", details=trans, expander_label="ì‚¬ìš©ì ìš”ì²­ ë‚´ì—­ ë³´ê¸°")
        _set_step("search"); st.rerun()

    if P["step"] == "search":
        if len(st.session_state.log_entries) < 2: logger.log(f"ğŸ” 2ë‹¨ê³„: '{P['trans']['target']}' ê°€ë§¹ì  ì •ë³´ ê²€ìƒ‰ ì¤‘...")
        agent._find_and_clarify_store()
        if "error" in agent.context: user_err(agent.context["error"]); P["running"] = False; _set_step("idle"); return
        if "clarification_needed" in agent.context:
            st.session_state.candidates = agent.context["clarification_needed"]
            st.session_state.awaiting_candidate = True
            logger.log("âš ï¸ 2ë‹¨ê³„: í›„ë³´ ê°€ë§¹ì  ë°œê²¬", details={"candidates": agent.context["clarification_needed"]}, expander_label="í›„ë³´ ê°€ë§¹ì  ëª©ë¡ ë³´ê¸°")
            return
        store_info = agent.context.get("target_store_info", {})
        logger.log(f"âœ… 2ë‹¨ê³„: '{store_info.get('ê°€ë§¹ì ëª…')}' ì •ë³´ í™•ì¸ ì™„ë£Œ", details=store_info, expander_label="í™•ì¸ëœ ê°€ë§¹ì  ì •ë³´ ë³´ê¸°")
        _set_step("analysis"); st.rerun()

    if P["step"] == "analysis":
        try:
            perform_analysis_sync(agent)
        except RuntimeError as e: 
             user_err(f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", details=agent.context)
             P["running"] = False; _set_step("idle"); return
        except Exception as e: 
             user_err(f"ì˜ˆìƒì¹˜ ëª»í•œ ë¶„ì„ ì˜¤ë¥˜: {e}", details=agent.context)
             P["running"] = False; _set_step("idle"); return

        # ê²°ê³¼ ì €ì¥ ë° ë¡œê·¸ ê¸°ë¡
        P["exploration"] = agent.context.get("exploration")
        P["timeseries_data"] = agent.context.get('target_store_timeseries')
        logger.log("âœ”ï¸ 3-1: ë‚´ë¶€/ì™¸ë¶€ ë°ì´í„° ë¶„ì„ ì™„ë£Œ", details=P["exploration"], expander_label="ë°ì´í„° ìš”ì•½ ê²°ê³¼ ë³´ê¸°")
        
        P["pd_out"] = agent.context.get("problem_definition")
        if not P["pd_out"] or "error" in P["pd_out"]: # None ì²´í¬ ì¶”ê°€
            user_err("AIê°€ ë¬¸ì œì ì„ ì •ì˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", details=P["pd_out"]); P["running"] = False; _set_step("idle"); return
        logger.log("âœ”ï¸ 3-2: í•µì‹¬ ë¬¸ì œì  ì •ì˜ ì™„ë£Œ", details=P["pd_out"], expander_label="ì •ì˜ëœ ë¬¸ì œì  ë³´ê¸°")

        report_md = agent._generate_strategy_and_report()
        if report_md.startswith("# ë³´ê³ ì„œ ì‹¤íŒ¨"):
             user_err("ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", details={"message": report_md})
        P["report_md"] = report_md
        logger.log("âœ”ï¸ 3-3: ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµ ìƒì„± ì™„ë£Œ")
        
        _set_step("done"); st.rerun()

    if P["step"] == "done":
        P["running"] = False
        _set_step("idle")
        st.rerun()

# --- ì…ë ¥ ë° ì œì–´ ë¡œì§ ---

# 1. Logger ê°ì²´ ìƒì„±
logger = StreamlitLogHandler()
user_query = st.chat_input("ê°€ë§¹ì  ì´ë¦„ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”.")

if user_query:
    st.session_state.messages.append(HumanMessage(content=user_query))
    with st.chat_message("user"): st.write(user_query)
    if st.session_state.pipeline.get("report_md"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."): reply = handle_followup(user_query)
        st.session_state.messages.append(AIMessage(content=reply)); st.rerun()
    else:
        st.session_state.log_entries = []
        st.session_state.pipeline.update({"report_md": None, "exploration": None, "pd_out": None, "trans": None, "timeseries_data": None}) # timeseries_data ì´ˆê¸°í™” ì¶”ê°€
        run_full_pipeline_resumable(user_text=user_query)

P = st.session_state.pipeline

if P.get("running"):
    with st.spinner("â³ AI ì—ì´ì „íŠ¸ê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        st.write("---"); st.subheader("ğŸ¤– AI ì—ì´ì „íŠ¸ ë¶„ì„ ê³¼ì •"); log_container = st.container(border=True)
        logger.render(log_container)
        if not st.session_state.awaiting_candidate: run_full_pipeline_resumable()
else:
    if st.session_state.get("log_entries"):
        st.write("---"); st.subheader("ğŸ¤– AI ì—ì´ì „íŠ¸ ë¶„ì„ ê³¼ì •"); log_container = st.container(border=True)
        logger.render(log_container)

    if P.get("report_md") and not P.get("report_md").startswith("# ë³´ê³ ì„œ ì‹¤íŒ¨"):
        # âœ… [ë””ë²„ê¹… ì¶”ê°€] ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        pd_out_exists = P.get("pd_out") is not None
        ts_data_exists = P.get("timeseries_data") is not None and not P.get("timeseries_data").empty

        if pd_out_exists and ts_data_exists:
            kpi_list = P["pd_out"].get("kpis", [])
            if kpi_list:
                st.subheader("ğŸ“Š ì£¼ìš” KPI ì‹œê³„ì—´ ì¶”ì´")
                charts = create_kpi_charts(kpi_list, P["timeseries_data"])
                if charts:
                    for chart in charts:
                        st.plotly_chart(chart, use_container_width=True)
                else:
                    st.info("ì„ ì •ëœ KPIì— ëŒ€í•œ ì‹œê³„ì—´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") 
        # ìµœì¢… ë³´ê³ ì„œ í‘œì‹œ
        st.subheader("ğŸ“˜ ìµœì¢… ë³´ê³ ì„œ (ìƒì„¸)"); st.markdown(P["report_md"], unsafe_allow_html=True)

    elif P.get("report_md"): # ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨ ë©”ì‹œì§€ í‘œì‹œ
         st.error(f"ë³´ê³ ì„œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {P['report_md']}")


if st.session_state.awaiting_candidate and st.session_state.candidates:
    st.info("ì´ë¦„ì´ ìœ ì‚¬í•œ ê°€ë§¹ì ì´ ì—¬ëŸ¬ ê°œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    cands = st.session_state.candidates
    choice_idx = st.radio("ë¶„ì„í•  ê°€ë§¹ì ì„ ì„ íƒí•˜ì„¸ìš”:", options=range(len(cands)), format_func=lambda i: _fmt_cand_with_id(cands[i]), label_visibility="collapsed")
    if st.button("ì„ íƒ í™•ì •", type="primary"):
        agent.context['target_store_info'] = cands[choice_idx]
        if 'clarification_needed' in agent.context: del agent.context['clarification_needed']
        st.session_state.awaiting_candidate = False
        _set_step("analysis")
        st.rerun()
    st.stop()

if P.get("running") and P.get("cancel_requested"):
    P.update({"running": False, "step": "idle", "cancel_requested": False}); st.info("â¹ï¸ ë¶„ì„ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
    st.rerun()