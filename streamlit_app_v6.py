import os
import sys
import json
import time
import shutil
import asyncio
import base64
import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import re
import html
import logging

from typing import Any, Dict, List
from pathlib import Path
from PIL import Image

from contextlib import AsyncExitStack

from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
from langchain_mcp_adapters.tools import load_mcp_tools
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

# =========================
# 0) ê³µí†µ ì„¤ì •
# =========================
ASSETS = Path("assets")
st.set_page_config(
    page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - ë§›ì§‘ì„ ìˆ˜í˜¸í•˜ëŠ” AIë¹„ë°€ìƒë‹´ì‚¬",
    layout="wide",
    page_icon="ğŸ“ˆ",
)

# âœ¨ ì œëª©/ë ˆì´ì•„ì›ƒ CSS: í•œêµ­ì–´ ì¤„ë°”ê¿ˆ, ë°˜ì‘í˜• í¬ê¸°, ì˜ë¦¼ ë°©ì§€
st.markdown("""
<style>
.block-container { padding-top: .8rem; padding-bottom: 1.2rem; max-width: 100%; }
.element-container, .stDataFrame { width: 100% !important; }

/* íƒ€ì´í‹€: í•œê¸€ ì¤„ë°”ê¿ˆ + ë°˜ì‘í˜• + ì˜ë¦¼ ë°©ì§€ */
.app-title{
  font-weight: 800;
  line-height: 1.25;
  font-size: clamp(22px, 2.2vw + 14px, 40px);
  margin: .2rem 0 1.0rem 0;
  white-space: normal;
  word-break: keep-all;
  overflow-wrap: anywhere;
}
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="app-title">ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì†Œ ğŸ”‘</h1>', unsafe_allow_html=True)
# ë¡œê¹…: í™”ë©´ ëŒ€ì‹  ì½˜ì†”ë¡œë§Œ
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)

def user_warn(msg: str):
    st.warning(msg, icon="âš ï¸")

def user_err(msg: str):
    st.error(msg, icon="ğŸ›‘")

# (í˜„ì¬ ì½”ë“œ ìœ ì§€â€”í•˜ë“œì½”ë“œ)
GOOGLE_API_KEY = "AIzaSyB8R3nurDOohfAvKXSgBUVRkoliXtfnTKo"
genai.configure(api_key=GOOGLE_API_KEY)

model = genai.GenerativeModel("gemini-2.5-flash")
async_model = genai.GenerativeModel("gemini-2.5-flash")

# ---------------------------
# ì—ì´ì „íŠ¸(ì¶”ë¡ /ë¦¬í¬íŒ…) ë¡œì§
# ---------------------------
class InteractiveParallelAgent:
    """
    - ë³€í™˜ â†’ ë¬¸ì œì •ì˜ â†’ ì „ëµ â†’ (í†µí•©)ìµœì¢… ë³´ê³ ì„œ
    - ë³´ê³ ì„œëŠ” í†µí•© í”„ë¡¬í”„íŠ¸(_GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT) ì‚¬ìš©
    - ì™¸ë¶€(run_full_pipeline)ì—ì„œ xploration(ì‹œê³„ì—´) ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
    """
        
    # 1) ë³€í™˜
    _TRANSFORM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìì—°ì–´ ìš”ì²­ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë„¤ ê°€ì§€ JSON í•­ëª©ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- "target": ì‚¬ìš©ìê°€ ì „ëµì„ ìš”ì²­í•œ ëŒ€ìƒì€ ë¬´ì—‡ì¸ê°€ìš”? (ê°€ë§¹ì  ì´ë¦„. ì˜ˆ: 'ì„±ìš°**', 'í–‰*') ('ë§¤ì¥', 'ê°€ë§¹ì 'ê³¼ ê°™ì€ ë‹¨ì–´ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”)
- "challenge": ì‚¬ìš©ìê°€ ì§ë©´í•œ ì–´ë ¤ì›€ì´ë‚˜ í•´ê²°í•´ì•¼ í•  ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?
- "objective": ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
- "solution_direction": ì œì•ˆë  ìˆ˜ ìˆëŠ” í•´ê²°ì±…ì˜ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€ìš”?
"""

    # 2) ë¬¸ì œ ì •ì˜
    _DEFINE_PROBLEM_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ê°€ë§¹ì ì˜ ì••ì¶•ëœ í†µê³„ ìš”ì•½ê³¼ ìµœì´ˆ ì‚¬ìš©ì ìš”ì²­ì„ ê¸°ë°˜ìœ¼ë¡œ, ë¬¸ì œ ìƒí™©ê³¼ í•µì‹¬ ì„±ê³¼ ì§€í‘œ(KPI)ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì •ì˜í•˜ì„¸ìš”.

**ë°ì´í„° êµ¬ì¡° ì„¤ëª…:**
ì œê³µëœ ë°ì´í„°ëŠ” ì‹œê³„ì—´ í†µê³„ ìš”ì•½ìœ¼ë¡œ, ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
1. ê¸°ë³¸ ì •ë³´ (meta): ê°€ë§¹ì ID, ì—…ì¢…, ìƒê¶Œ, ë¶„ì„ê¸°ê°„, cluster ì •ë³´
2. ì „ì²´ í‰ê·  ì§€í‘œ (avg_metrics): 24ê°œì›” í‰ê· ê°’
3. íŠ¸ë Œë“œ (trend): ì´ˆê¸° 6ê°œì›” vs ìµœê·¼ 6ê°œì›” ë¹„êµ
4. ìµœê·¼ 3ê°œì›” ìƒì„¸ (recent_3months): ì›”ë³„ ìƒì„¸ ë°ì´í„°
5. ì£¼ìš” ì´ë²¤íŠ¸ (significant_events): ê¸‰ê²©í•œ ë³€í™” ì‹œì 
6. êµ¬ê°„ ë¹ˆë„ (range_frequency): ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ì„±ê³¼ êµ¬ê°„

**ë°ì´í„° í•´ì„ ê°€ì´ë“œ:**
- ë§¤ì¶œê¸ˆì•¡/ê±´ìˆ˜/ê³ ê°ìˆ˜: ì‹¤ì œ ê°’
- ì·¨ì†Œìœ¨: ì‹¤ì œ ë¹„ìœ¨
- ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡/ê±´ìˆ˜ ë¹„ìœ¨: 100% ê¸°ì¤€, 100% ì´ìƒì´ë©´ í‰ê·  ì´ìƒ
- ë™ì¼ ì—…ì¢…/ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨: 0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„ê¶Œ
- cluster: 0(ìƒìœ„ê¶Œ ì•ˆì •), 1(í•˜ìœ„ê¶Œ ì•ˆì •), 2(ë³€ë™ì„± ë†’ìŒ), 3(ì‹ ê·œ ê°€ë§¹ì )
- cluster_6: 0(ì—¬ì„±40ëŒ€+ê±°ì£¼), 1(ë‚¨ì„±30-40ëŒ€+ì‹ ê·œ), 2(2030+ì‹ ê·œ+ìœ ë™), 3(60ëŒ€ì´ìƒ), 4(ë‚¨ì„±40-60ëŒ€), 5(ì§ì¥+ë‚¨ì„±30ëŒ€+ì¬ë°©ë¬¸)

**íŠ¹íˆ ì£¼ëª©í•  ì :**
- íŠ¸ë Œë“œì˜ ë³€í™”ìœ¨(ì´ˆê¸° ëŒ€ë¹„ ìµœê·¼)
- ìµœê·¼ 3ê°œì›”ì˜ ì›”ë³„ ì¶”ì„¸
- ì£¼ìš” ì´ë²¤íŠ¸(ê¸‰ë“±/ê¸‰ë½) ì‹œì ê³¼ í¬ê¸°

ê²°ê³¼ëŠ” ë‹¤ìŒ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- "problem_statement": í˜„ì¬ ì§ë©´í•œ êµ¬ì²´ì ì¸ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€ìš”? (íŠ¸ë Œë“œì™€ ìµœê·¼ ë°ì´í„° ê¸°ë°˜)
- "kpis": ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì¸¡ì • ê°€ëŠ¥í•œ í•µì‹¬ ì§€í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”? (ë¬¸ìì—´ ëª©ë¡)
"""

    # 4) ì „ëµ ì œì•ˆ
    _GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ë›°ì–´ë‚œ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì´ì ì „ë¬¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ 'ë¬¸ì œ ì •ì˜'ì™€ ì••ì¶•ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬, ìµœì¢… ì‚¬ìš©ìë¥¼ ìœ„í•œ í¬ê´„ì ì¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ë³´ê³ ì„œëŠ” ë‹¤ìŒì˜ ëª…í™•í•œ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì„œë¡ : ë¬¸ì œ ë°°ê²½ ë° ë¶„ì„ ëª©ì ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.

ë³¸ë¡ :

ë°ì´í„° ê¸°ë°˜ í˜„í™© ë¶„ì„: íŠ¸ë Œë“œì™€ ìµœê·¼ 3ê°œì›” ë°ì´í„°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•©ë‹ˆë‹¤.

í•µì‹¬ ë¬¸ì œ ì •ì˜: ë¶„ì„ì„ í†µí•´ ë„ì¶œëœ ê°€ì¥ ì‹œê¸‰í•˜ê³  ì¤‘ìš”í•œ ë¬¸ì œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

êµ¬ì²´ì ì¸ í•´ê²° ì „ëµ: ì •ì˜ëœ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµê³¼ ì„¸ë¶€ ì‹¤í–‰ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤. ë˜í•œ, ì™œ ì´ ì „ëµì´ íš¨ê³¼ì ì¼ì§€ì— ëŒ€í•œ ë…¼ë¦¬ì  ê·¼ê±°ë¥¼ ë°ì´í„° ì¶”ì„¸ì™€ ì—°ê²°í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.

ê²°ë¡ : ì œì•ˆëœ ì „ëµ ì‹¤í–‰ ì‹œ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ "report"ë¼ëŠ” ë‹¨ì¼ í‚¤ë¥¼ ê°€ì§„ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°, ê°’ì—ëŠ” ì „ì²´ ë³´ê³ ì„œ ë‚´ìš©ì´ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
ì˜ˆì‹œ: {"report": "## ìµœì¢… ë³´ê³ ì„œ\n\n### 1. ê°œìš”\n...\n### 2. í˜„í™© ë¶„ì„ ë° ë¬¸ì œ ì •ì˜\nìµœê·¼ 3ê°œì›”ê°„ ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘ì´ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤...\n### 3. í•´ê²° ì „ëµ\n...\n"}
"""

    def __init__(self, df: pd.DataFrame, model):
        self.df = df
        self.model = model
        self.context = {} # ë¶„ì„ ê³¼ì •ì„ ì €ì¥í•  ì»¨í…ìŠ¤íŠ¸

    def _generate_content_sync(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """
        LLM í˜¸ì¶œì„ ë‹´ë‹¹í•˜ëŠ” ì¤‘ì•™ ë©”ì„œë“œ. max_output_tokens ë“± ë‹¤ì–‘í•œ ì˜µì…˜ ì§€ì›.
        """
        try:
            # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: generation_configë¥¼ ë™ì ìœ¼ë¡œ ì„¤ì •
            generation_config = {"response_mime_type": "application/json"}
            if 'max_output_tokens' in kwargs:
                generation_config['max_output_tokens'] = kwargs['max_output_tokens']

            resp = self.model.generate_content(
                prompt,
                generation_config=generation_config
            )
            return json.loads(resp.text)
        except Exception as e:
            return {"error": f"LLM ë™ê¸° í˜¸ì¶œ ì‹¤íŒ¨: {e.__class__.__name__}: {e}"}

    def transform(self, initial_input: str):
        user = f"ğŸ”¹ ì‚¬ìš©ì ì…ë ¥: {initial_input}"
        full = f"{self._TRANSFORM_SYSTEM_PROMPT}\n\n{user}"
        self.context["transformation"] = self._generate_content_sync(full)
        return self.context["transformation"]
    
    def set_target_store(self, merchant_id: str):
        """ë¶„ì„í•  ëŒ€ìƒ ê°€ë§¹ì ì˜ IDë¥¼ ë°›ì•„ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³  ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤."""
        store_data = self.df[self.df['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸'].astype(str) == str(merchant_id)]
        if store_data.empty:
            self.context['error'] = "í•´ë‹¹ IDì˜ ê°€ë§¹ì  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return
        # ë¶„ì„ì— ì‚¬ìš©ë  ê°€ë§¹ì ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.
        self.context['target_store_timeseries'] = store_data.sort_values(by='ê¸°ì¤€ë…„ì›”').reset_index(drop=True)

    def compress_store_data(self) -> Dict[str, Any]:
        """
        âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: 3ë‹¨ê³„: ê°€ë§¹ì  ë°ì´í„° ì••ì¶• ìš”ì•½ (ì‚¬ìš©ì ì œê³µ ë¡œì§ í†µí•©)
        """
        print("3ë‹¨ê³„: ë°ì´í„° ì••ì¶• ìš”ì•½ ì‹œì‘...")
        store_data = self.context.get('target_store_timeseries')
        if store_data is None or store_data.empty:
            self.context['error'] = "ì••ì¶•í•  ê°€ë§¹ì  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
            return {"error": self.context['error']}

        # --- (ì‚¬ìš©ìê°€ ì œê³µí•œ ì••ì¶• ë¡œì§ ì‹œì‘) ---
        first_row = store_data.iloc[0]
        meta = {
            'ê°€ë§¹ì ID': str(first_row.get('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸')), 'ê°€ë§¹ì ëª…': first_row.get('ê°€ë§¹ì ëª…'),
            'ì—…ì¢…': first_row.get('ì—…ì¢…'), 'ìƒê¶Œ': first_row.get('ìƒê¶Œ'),
            'ë¶„ì„ê¸°ê°„': f"{store_data['ê¸°ì¤€ë…„ì›”'].min().strftime('%Y-%m')} ~ {store_data['ê¸°ì¤€ë…„ì›”'].max().strftime('%Y-%m')}",
            'ë°ì´í„°í¬ì¸íŠ¸': len(store_data),
            'cluster': int(first_row['cluster']) if pd.notna(first_row.get('cluster')) else None,
            'cluster_6': int(first_row['cluster_6']) if pd.notna(first_row.get('cluster_6')) else None
        }
        numeric_cols = [
            'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨','ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨', 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œ ìˆœìœ„ ë¹„ìœ¨',
            'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘', 'ê±°ì£¼ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'
        ] # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        
        store_data_clean = store_data.copy()
        for col in numeric_cols:
            if col in store_data_clean.columns:
                store_data_clean[col] = pd.to_numeric(store_data_clean[col], errors='coerce')
        
        avg_metrics = {col: round(store_data_clean[col].mean(), 2) for col in numeric_cols if col in store_data_clean.columns and store_data_clean[col].notna().any()}
        
        first_6 = store_data_clean.head(6)
        last_6 = store_data_clean.tail(6)
        trend = {}
        for col in numeric_cols:
            if col in store_data_clean.columns:
                first_val, last_val = first_6[col].mean(), last_6[col].mean()
                if pd.notna(first_val) and pd.notna(last_val) and first_val != 0:
                    change_pct = ((last_val - first_val) / abs(first_val)) * 100
                    trend[col] = {'ì´ˆê¸°_í‰ê· ': round(first_val, 2), 'ìµœê·¼_í‰ê· ': round(last_val, 2), 'ë³€í™”ìœ¨': f"{change_pct:+.1f}%"}
        # --- (ì‚¬ìš©ìê°€ ì œê³µí•œ ì••ì¶• ë¡œì§ ì¢…ë£Œ, ì¼ë¶€ ê°„ì†Œí™”) ---

        exploration_result = {'meta': meta, 'avg_metrics': avg_metrics, 'trend': trend}
        self.context['exploration'] = exploration_result
        print("3ë‹¨ê³„: ë°ì´í„° ì••ì¶• ì™„ë£Œ.")
        return exploration_result
    
    def define_problem(self) -> Dict[str, Any]:
        """
        âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: 4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ (ì••ì¶• ë°ì´í„°ì™€ max_tokens ì‚¬ìš©)
        """
        print("4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì‹œì‘...")
        exploration = self.context.get('exploration')
        transformation = self.context.get('transformation')
        if not exploration or not transformation:
            return {"error": "ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
            
        user_prompt = f"""
ğŸ”¹ ë¬¸ë§¥(Context):
- ìµœì´ˆ ìš”ì²­: {json.dumps(transformation, indent=2, ensure_ascii=False)}
- ì••ì¶•ëœ ë°ì´í„° ë¶„ì„ ê²°ê³¼: {json.dumps(exploration, indent=2, ensure_ascii=False)}
"""
        full_prompt = f"{self._DEFINE_PROBLEM_SYSTEM_PROMPT}\n\n{user_prompt}"
        
        response = self._generate_content_sync(
            full_prompt,
            max_output_tokens=4096  # âœ… max_tokens ì ìš©
        )
        self.context['problem_definition'] = response
        print("4ë‹¨ê³„: ë¬¸ì œ ì •ì˜ ì™„ë£Œ.")
        return response

    def generate_report(self) -> str:
        """5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
        compact_context = {
            "transformation": self.context.get("transformation"),
            "problem_definition": self.context.get("problem_definition"),
            "exploration_summary": self.context.get("exploration"),
        }
        user_prompt = "ğŸ”¹ ì „ì²´ ì»¨í…ìŠ¤íŠ¸:\n" + json.dumps(compact_context, ensure_ascii=False, indent=2)
        full_prompt = f"{self._GENERATE_STRATEGY_REPORT_SYSTEM_PROMPT}\n\n{user_prompt}"

        response = self._generate_content_sync(full_prompt,
        max_output_tokens=8192, # (ì°¸ê³ ) Flash ëª¨ë¸ì˜ ìµœëŒ€ ì¶œë ¥ì€ 8192 í† í°ì…ë‹ˆë‹¤.
        temperature=0.3
    )
        
        if "report" in response:
            print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ.")
            return response["report"]
        else:
            print("5ë‹¨ê³„: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨ (Fallback).")
            # Fallback: ì‹¤íŒ¨ ì‹œì—ë„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ì—¬ì¤Œ
            return f"# ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨\n\nAIê°€ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ë¶„ì„ì— ì‚¬ìš©ëœ ë°ì´í„° ìš”ì•½ì…ë‹ˆë‹¤.\n\n```json\n{json.dumps(compact_context, ensure_ascii=False, indent=2)}\n```"

# ---------------------------
# Streamlit UI
# ---------------------------
system_prompt = (
    "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë§ˆì¼€íŒ… ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê°€ë§¹ì ëª…ì„ ë°›ì•„ í•´ë‹¹ ê°€ë§¹ì ì˜ ë°©ë¬¸ ê³ ê° í˜„í™©ì„ ë¶„ì„í•˜ê³ , "
    "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë§ˆì¼€íŒ… ë°©ë²•ê³¼ ì±„ë„, ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. "
    "ê²°ê³¼ëŠ” ì§§ê³  ê°„ê²°í•˜ê²Œ, ë¶„ì„ ê²°ê³¼ì—ëŠ” ê°€ëŠ¥í•œ í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
greeting = "ë§ˆì¼€íŒ…ì´ í•„ìš”í•œ ê°€ë§¹ì ì„ ì•Œë ¤ì£¼ì„¸ìš”."

# ===== ì„¸ì…˜ ìƒíƒœ ì•ˆì „ ì´ˆê¸°í™” =====
def ensure_state():
    ss = st.session_state
    if "messages" not in ss:
        ss.messages = [
            SystemMessage(content=system_prompt),
            AIMessage(content=greeting),
        ]

    ss.setdefault("awaiting_candidate", False)
    ss.setdefault("candidates", [])
    ss.setdefault("exploration", None)
    ss.setdefault("transformation", None)
    ss.setdefault("selected_merchant_id", None)
    ss.setdefault("user_query", "")

    # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: ì°¨íŠ¸ ê´€ë ¨ ìƒíƒœ(chart_b64, used_kpis) ì œê±°
    if "pipeline" not in ss:
        ss.pipeline = {
            "running": False,
            "cancel_requested": False,
            "step": "idle",
            "merchant_id": None,
            "user_query": "",
            "trans": None,
            "candidates": [],
            "exploration": None,
            "pd_out": None,
            "report_md": None,
        }

ensure_state()

@st.cache_data
def load_image(name: str):
    return Image.open(ASSETS / name)

def clear_chat_history():
    st.session_state.messages = [
        SystemMessage(content=system_prompt),
        AIMessage(content=greeting),
    ]
    st.session_state.awaiting_candidate = False
    st.session_state.candidates = []
    st.session_state.exploration = None
    st.session_state.transformation = None
    st.session_state.selected_merchant_id = None
    st.session_state.user_query = ""
    # âœ… ìš”êµ¬ì‚¬í•­ ë°˜ì˜: íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    st.session_state.pipeline = {
        "running": False, "cancel_requested": False, "step": "idle",
        "merchant_id": None, "user_query": "", "trans": None,
        "candidates": [], "exploration": None, "pd_out": None,
        "report_md": None,
    }

def handle_followup(user_question: str) -> str:
    """
    í˜„ì¬ ì„¸ì…˜ì˜ exploration / transformation / reportë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•´
    ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. ìƒˆ íŒŒì´í”„ë¼ì¸ì„ ëŒë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    exp = st.session_state.pipeline.get("exploration")
    trans = st.session_state.pipeline.get("trans")
    report_md = st.session_state.pipeline.get("report_md")

    ctx = {
        "store_identity": (exp or {}).get("store_identity"),
        "time_series_tail": (exp or {}).get("time_series_analysis_data", [])[-12:] if exp else [],
        "transformation": trans,
        "report": report_md,
    }
    prompt = (
        "ë‹¹ì‹ ì€ ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ë‹µí•˜ì„¸ìš”.\n"
        "ìˆ«ìëŠ” í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ ëª…í™•íˆ, ëª¨ë¥´ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ë¶€ì¡±í•˜ë‹¤ê³  ë°í˜€ì£¼ì„¸ìš”.\n\n"
        f"### ì»¨í…ìŠ¤íŠ¸\n{json.dumps(ctx, ensure_ascii=False, indent=2)}\n\n"
        f"### ì‚¬ìš©ìì˜ ì§ˆë¬¸\n{user_question}\n\n"
        "### ë‹µë³€"
    )
    try:
        resp = model.generate_content(prompt)
        return resp.text.strip()
    except Exception as e:
        return f"ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# í›„ë³´ í‘œì‹œ í¬ë§·
def _id_of(cand: dict):
    return cand.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸")

def _fmt_cand_with_id(c: dict) -> str:
    id_val = _id_of(c) or "UNKNOWN_ID"
    area = c.get("ìƒê¶Œ", "ìƒê¶Œ ì •ë³´ ì—†ìŒ")
    name = c.get("ê°€ë§¹ì ëª…", "?")
    industry = c.get("ì—…ì¢…", "?")
    return f"[{id_val}] {name} / {industry} / {area}"

def _set_step(step: str):
    st.session_state.pipeline["step"] = step

def _cancel_requested() -> bool:
    return bool(st.session_state.pipeline.get("cancel_requested"))

def _abort_if_cancelled():
    if _cancel_requested():
        st.info("â¹ï¸ ì „ëµ ì œì•ˆì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
        st.session_state.pipeline["running"] = False
        _set_step("idle")
        raise st.stop()

# === ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ê¸°ì¡´ ë¸”ë¡ì— ì¶”ê°€) ===
# if "pipeline" not in st.session_state:
#     st.session_state.pipeline = {
#         "running": False,            # íŒŒì´í”„ë¼ì¸ ë™ì‘ ì—¬ë¶€
#         "cancel_requested": False,   # ì¤‘ì§€ ìš”ì²­ í”Œë˜ê·¸
#         "step": "idle",              # í˜„ì¬ ë‹¨ê³„
#         "merchant_id": None,
#         "user_query": "",
#         # ì‚°ì¶œë¬¼ ìºì‹œ(ì¬ì‹¤í–‰ ì‹œ ì´ì–´ê°€ê¸° ìœ„í•¨)
#         "trans": None,
#         "candidates": [],
#         "exploration": None,
#         "pd_out": None,
#         "chart_b64": None,
#         "used_kpis": None,
#         "report_md": None,
#     }

# =========================
# 5) ì‚¬ì´ë“œë°”
# =========================
with st.sidebar:
    logo = ASSETS / "shc_ci_basic_00.png"
    if logo.exists():
        st.image(load_image("shc_ci_basic_00.png"), use_container_width=True)
    st.markdown("<p style='text-align: center;'>2025 Big Contest â€¢ AI DATA í™œìš©</p>", unsafe_allow_html=True)
    st.button("Clear Chat History", on_click=clear_chat_history)

    # ğŸ”´ ì§„í–‰ ì¤‘ ì¤‘ì§€ ë²„íŠ¼ (ì¬ì‹¤í–‰ë˜ì–´ë„ 'running' ìœ ì§€)
    col_stop = st.container()
    if st.session_state.pipeline["running"]:
        if col_stop.button("ë¶„ì„ ì¤‘ë‹¨", type="secondary"):
            st.session_state.pipeline["cancel_requested"] = True
        
# --- íˆìŠ¤í† ë¦¬ ë Œë” ---
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        with st.chat_message("user"):
            st.write(msg.content)
    elif isinstance(msg, AIMessage):
        with st.chat_message("assistant"):
            st.write(msg.content)

# --- ì‹œê³„ì—´ ë Œë” ---
@st.cache_data(show_spinner=False)
def _prepare_ts(ts_list: list) -> pd.DataFrame:
    df = pd.DataFrame(ts_list)
    if "ê¸°ì¤€ë…„ì›”" in df.columns:
        try:
            df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(df["ê¸°ì¤€ë…„ì›”"])
        except Exception:
            pass
        df = df.sort_values("ê¸°ì¤€ë…„ì›”")
    return df.reset_index(drop=True)

def render_timeseries(exploration: dict, merchant_id: str | None = None):
    ts = exploration.get("time_series_analysis_data") or []
    if not ts:
        st.info("ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    df = _prepare_ts(ts)
    st.subheader("ğŸ“Š ìµœê·¼ 12ê°œì›” ë°ì´í„° ìš”ì•½")
    st.dataframe(df.tail(12), use_container_width=True, key=f"ts_tail_{merchant_id or 'na'}")

# --- ì „ì²´ íŒŒì´í”„ë¼ì¸ ---
# âœ… ë°ì´í„° ë¡œë”© ë° Agent ì´ˆê¸°í™” (ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ)
@st.cache_resource
def load_data_and_init_agent():
    # ë°ì´í„° ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.
    try:
        # ì´ ë¶€ë¶„ì€ mcp_serverì˜ _load_df ë¡œì§ê³¼ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        file_path = "./data/labeling_no_preprocessing.csv"
        df = pd.read_csv(file_path)
        df.columns = df.columns.str.strip()
        df["ê¸°ì¤€ë…„ì›”"] = pd.to_datetime(df["ê¸°ì¤€ë…„ì›”"].astype(str).str.zfill(6), format="%Y%m")
        agent = InteractiveParallelAgent(df, model)
        return agent, df
    except FileNotFoundError:
        st.error(f"ë°ì´í„° íŒŒì¼({file_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        st.stop()

agent, df = load_data_and_init_agent()

# âœ… mcp_serverë¥¼ í˜¸ì¶œí•˜ëŠ” ëŒ€ì‹ , Agentê°€ ì§ì ‘ ê²€ìƒ‰í•˜ë„ë¡ ë³€ê²½
def search_merchant_local(store_name: str) -> Dict[str, Any]:
    search_term = (store_name or "").replace("*", "")
    hits = df[df["ê°€ë§¹ì ëª…"].astype(str).str.contains(search_term, case=False, na=False)]
    if hits.empty:
        return {"status": "error", "message": f"'{search_term}'ë¡œ ê²€ìƒ‰ëœ ê°€ë§¹ì ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    cand_cols = ["ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸", "ê°€ë§¹ì ëª…", "ì—…ì¢…", "ìƒê¶Œ", "ê°€ë§¹ì ì£¼ì†Œ"]
    cands = hits[cand_cols].drop_duplicates().to_dict(orient="records")

    if len(cands) == 1:
        return {"status": "single_candidate", "candidate": cands[0]}
    return {"status": "clarification_needed", "candidates": cands}


def run_full_pipeline_resumable(user_text: str | None = None, merchant_id: str | None = None):
    P = st.session_state.pipeline
    
    if not P["running"]: # ìµœì´ˆ ì‹¤í–‰
        if user_text is None: return
        P.update({"running": True, "cancel_requested": False, "step": "transform", "user_query": user_text})

    if P["step"] == "transform":
        trans = agent.transform(P["user_query"])
        # âœ… ë‚´ìš© ê²€ì¦: 'target' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        if "error" in trans or not trans.get("target"):
            user_err("ìš”ì²­ì„ ë¶„ì„í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë” ëª…í™•í•œ ê°€ë§¹ì  ì´ë¦„ì„ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            P["running"] = False; _set_step("idle"); return
        P["trans"] = trans
        _set_step("search")

    if P["step"] == "search":
        target = P["trans"]["target"]
        resp = search_merchant_local(target) # ë¡œì»¬ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
        
        if resp["status"] == "error":
            user_err(resp["message"])
            P["running"] = False; _set_step("idle"); return

        if resp["status"] == "clarification_needed":
            st.session_state.candidates = resp["candidates"]
            st.session_state.awaiting_candidate = True
            return # ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸°

        if resp["status"] == "single_candidate":
            P["merchant_id"] = resp["candidate"]["ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"]
            _set_step("analysis")

    if P["step"] == "analysis":
        with st.spinner("â³ AIê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            agent.set_target_store(P["merchant_id"])
            exploration = agent.compress_store_data()
            if "error" in exploration:
                user_err(f"ë°ì´í„° ìš”ì•½ ì¤‘ ì˜¤ë¥˜: {exploration['error']}")
                P["running"] = False; _set_step("idle"); return

            pd_out = agent.define_problem()
            # âœ… ë‚´ìš© ê²€ì¦: 'problem_statement' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
            if "error" in pd_out or not pd_out.get("problem_statement"):
                user_err("AIê°€ ë¬¸ì œì ì„ ì •ì˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                P["running"] = False; _set_step("idle"); return

            report_md = agent.generate_report()
            # âœ… ë‚´ìš© ê²€ì¦: ê²°ê³¼ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            if not report_md or "ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨" in report_md:
                user_err("ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                # ì‹¤íŒ¨ ì‹œì—ë„ ìƒì„±ëœ report_md(Fallback ë©”ì‹œì§€)ëŠ” ê·¸ëŒ€ë¡œ í‘œì‹œ
            
            P["report_md"] = report_md
        _set_step("done")

    if P["step"] == "done":
        st.subheader("ğŸ“˜ ìµœì¢… ë³´ê³ ì„œ (ìƒì„¸)")
        st.markdown(P["report_md"], unsafe_allow_html=True)
        # ìš”ì•½ ë©”ì‹œì§€ëŠ” ìƒëµí•˜ê³  ìµœì¢… ë³´ê³ ì„œë§Œ ë°”ë¡œ í‘œì‹œ
        P["running"] = False
        _set_step("idle")

# --- ì…ë ¥ì°½ ---
# --- ì…ë ¥ ë° ì œì–´ ë¡œì§ ---
user_query = st.chat_input("ê°€ë§¹ì  ì´ë¦„ì´ë‚˜ ê³ ë¯¼ì„ ì…ë ¥í•˜ì„¸ìš”.")
if user_query:
    st.session_state.user_query = user_query
    st.session_state.messages.append(HumanMessage(content=user_query))
    with st.chat_message("user"):
        st.write(user_query)

    if (not st.session_state.pipeline["running"]) and st.session_state.pipeline.get("report_md"):
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            followup_response = handle_followup(user_query)
        st.session_state.messages.append(AIMessage(content=followup_response))
        with st.chat_message("assistant"):
            st.markdown(followup_response)
    elif not st.session_state.pipeline["running"]:
        run_full_pipeline_resumable(user_query, merchant_id=None)

if st.session_state.awaiting_candidate and st.session_state.candidates:
    st.info("ì´ë¦„ì´ ìœ ì‚¬í•œ ê°€ë§¹ì ì´ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤. ì•„ë˜ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    cands = st.session_state.candidates
    idx = st.radio("ê°€ë§¹ì ì„ ì„ íƒí•˜ì„¸ìš”",
                   options=list(range(len(cands))),
                   format_func=lambda i: _fmt_cand_with_id(cands[i]),
                   key="cand_radio",
                   label_visibility="collapsed")

    if st.button("ì„ íƒ í™•ì •", key="cand_confirm", type="primary"):
        sel = cands[idx]
        picked_id = str(sel.get("ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸"))
        st.session_state.awaiting_candidate = False
        st.session_state.pipeline["merchant_id"] = picked_id
        _set_step("analysis")
        st.rerun()

    st.stop()

if st.session_state.pipeline["running"] and st.session_state.pipeline.get("cancel_requested"):
    st.session_state.pipeline.update({"running": False, "step": "idle", "cancel_requested": False})
    st.info("â¹ï¸ ë¶„ì„ì„ ì¤‘ì§€í–ˆìŠµë‹ˆë‹¤.")
    st.rerun()

if st.session_state.pipeline["running"] and not st.session_state.awaiting_candidate:
    run_full_pipeline_resumable()